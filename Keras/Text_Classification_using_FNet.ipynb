{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification using FNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQOQAu1GrrbIfJOYU+0Qag"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this example, we will demonstrate the ability of FNet to achieve comparable results with a vanilla Transformer model on the text classification task. We will be using the IMDb dataset, which is a collection of movie reviews labelled either positive or negative (sentiment analysis).\n",
        "\n",
        "To build the tokenizer, model, etc., we will use components from KerasNLP. KerasNLP makes life easier for people who want to build NLP pipelines! :)\n",
        "\n",
        "# Model\n",
        "Transformer-based language models (LMs) such as BERT, RoBERTa, XLNet, etc. have demonstrated the effectiveness of the self-attention mechanism for computing rich embeddings for input text. However, the self-attention mechanism is an expensive operation, with a time complexity of O(n^2), where n is the number of tokens in the input. Hence, there has been an effort to reduce the time complexity of the self-attention mechanism and improve performance without sacrificing the quality of results.\n",
        "\n",
        "In 2020, a paper titled FNet: Mixing Tokens with Fourier Transforms replaced the self-attention layer in BERT with a simple Fourier Transform layer for \"token mixing\". This resulted in comparable accuracy and a speed-up during training. In particular, a couple of points from the paper stand out:\n",
        "\n",
        "* The authors claim that FNet is 80% faster than BERT on GPUs and 70% faster on TPUs. The reason for this speed-up is two-fold: a) the Fourier Transform layer is unparametrized, it does not have any parameters, and b) the authors use Fast Fourier Transform (FFT); this reduces the time complexity from O(n^2) (in the case of self-attention) to O(n log n).\n",
        "* FNet manages to achieve 92-97% of the accuracy of BERT on the GLUE benchmark."
      ],
      "metadata": {
        "id": "CIRpwKYjUZwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "kycNcdaqU-w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-nlp"
      ],
      "metadata": {
        "id": "7QJr5YDFVKBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "\n",
        "keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "Bf2JEfmKVBMO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparams"
      ],
      "metadata": {
        "id": "RP0-ZuDgVDCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 512"
      ],
      "metadata": {
        "id": "9ZtQHzm0VEjk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tWAFOOGYVwiE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}