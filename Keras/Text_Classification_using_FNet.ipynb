{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification using FNet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMcisrJwNUPy1c0sUK3pmUt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this example, we will demonstrate the ability of FNet to achieve comparable results with a vanilla Transformer model on the text classification task. We will be using the IMDb dataset, which is a collection of movie reviews labelled either positive or negative (sentiment analysis).\n",
        "\n",
        "To build the tokenizer, model, etc., we will use components from KerasNLP. KerasNLP makes life easier for people who want to build NLP pipelines! :)\n",
        "\n",
        "# Model\n",
        "Transformer-based language models (LMs) such as BERT, RoBERTa, XLNet, etc. have demonstrated the effectiveness of the self-attention mechanism for computing rich embeddings for input text. However, the self-attention mechanism is an expensive operation, with a time complexity of O(n^2), where n is the number of tokens in the input. Hence, there has been an effort to reduce the time complexity of the self-attention mechanism and improve performance without sacrificing the quality of results.\n",
        "\n",
        "In 2020, a paper titled FNet: Mixing Tokens with Fourier Transforms replaced the self-attention layer in BERT with a simple Fourier Transform layer for \"token mixing\". This resulted in comparable accuracy and a speed-up during training. In particular, a couple of points from the paper stand out:\n",
        "\n",
        "* The authors claim that FNet is 80% faster than BERT on GPUs and 70% faster on TPUs. The reason for this speed-up is two-fold: a) the Fourier Transform layer is unparametrized, it does not have any parameters, and b) the authors use Fast Fourier Transform (FFT); this reduces the time complexity from O(n^2) (in the case of self-attention) to O(n log n).\n",
        "* FNet manages to achieve 92-97% of the accuracy of BERT on the GLUE benchmark."
      ],
      "metadata": {
        "id": "CIRpwKYjUZwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "kycNcdaqU-w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-nlp"
      ],
      "metadata": {
        "id": "7QJr5YDFVKBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "\n",
        "keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "Bf2JEfmKVBMO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparams"
      ],
      "metadata": {
        "id": "RP0-ZuDgVDCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 512"
      ],
      "metadata": {
        "id": "9ZtQHzm0VEjk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "First, let's download the IMDB dataset and extract it."
      ],
      "metadata": {
        "id": "tWAFOOGYVwiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "id": "DpIaCxeQWCdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please refer to [Text Classification from Scratch](https://github.com/suvigyajain0101/NLP/blob/main/Keras/Text_classification_from_scratch.ipynb) for data exploration on the IMDB dataset"
      ],
      "metadata": {
        "id": "J_apj0JHWEL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the keras.utils.text_dataset_from_directory utility to generate our labelled tf.data.Dataset dataset from text files."
      ],
      "metadata": {
        "id": "Q4VX3CuBX6YU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH5sxb9PXJxg",
        "outputId": "514de68e-cbe2-44da-b4fa-41f2e64c3e69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 75000 files belonging to 3 classes.\n",
            "Using 60000 files for training.\n",
            "Found 75000 files belonging to 3 classes.\n",
            "Using 15000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now convert the text to lowercase."
      ],
      "metadata": {
        "id": "MqbQoyt8XnRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(lambda x, y: (tf.strings.lower(x), y))\n",
        "val_ds = val_ds.map(lambda x, y: (tf.strings.lower(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (tf.strings.lower(x), y))"
      ],
      "metadata": {
        "id": "lXuPwcUPX-M3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print a few samples."
      ],
      "metadata": {
        "id": "iRAaDZkqX-nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp6hQhXaYCjI",
        "outputId": "87cc666c-31b9-4598-929c-b6cd9b2993a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'i found this dvd at big lots for $2. with badly photoshopped images of dennis hopper and michael madsen (obviously from a reservoir dogs promo) on the cover, i had very low expectations. but i\\'ll give this movie credit. at least the director has some idea of what he was doing. unlike other crap films like \"cheerleader ninjas\" and \"tha sistahood\", this one could actually direct the camera in a way that didn\\'t make you want to blow your brains out. but when saying the director wasn\\'t doing hits off of a crack pipe while working is the best compliment to give a movie, that\\'s still pretty bad.<br /><br />the story is basically about how corrupt the lapd is. this corruption is basically revolving around a half dozen or so cops. that\\'s pretty well contained, i must say. there\\'s also a subplot of guys robbing convenience stores that goes nowhere. and another subplot of the main guy\\'s love affair with a cop groupie. that one may be uninspired, but the sex scene is probably the funniest i\\'ve ever seen in my life. the two roll around naked under covers in the most awkward positions possible while the guy grinds his loins and gives the most painful grimaces is the history of man. honestly, it just has to be seen to believed.<br /><br />casting sucks. dennis hopper is in the movie for a total of about 10 minutes, and looks like he\\'s in physical pain having to deliver the insultingly bad script. madsen is in there for more and seems less affected, but he\\'s no stranger to a bad script. i\\'ve sat through films with him that were much worse, so this is nothing new to him. the hot young cop who owns the nickname cowboy (because, you know, they want to be original) looks about 40. one fatboy cop, who looks mid-thirties, has a non-touching scene with his \"mother\" of 45. the constant unexplained parties at fatboy\\'s \"mansion\" have the same people who seem not to change their clothes for weeks on end.<br /><br />but the worst part of this film is the logic. cops w/ bullet proof vests take shots like superman without barely a flinch. whenever there\\'s a shootout within 20 square feet, nobody considers taking cover from the bullets. and why should they? trained lapd officers are known for being terrible shots anyway, right? there\\'s also no sense in ballistics when detectives come around.<br /><br />bottom line, this film may suck noodles but i\\'ve seen madsen in worse (against all hope, executive target, my boss\\'s daughter, etc). unless you\\'re a big fan of his, as i am, stay away from this film.'\n",
            "2\n",
            "b'i think the croc hunter is a pretty cool guy! i know i wouldn\\'t have the nerve to go even 5 feet away from a croc.<br /><br />but, everything in this movie is bad. farting jokes, people getting eaten, and the skit about the president all make the movie one of the worst of all time.<br /><br />it\\'s a really bad film that you have to stay away from. all the \"jokes\" are so juvenile that you will find yourself laughing because they are so stupid. the plot is so bad that you wonder if the screenwriter is 4 years old.<br /><br />i\\'m surprised the croc hunter did not beg the crocodile to eat him after he saw this.'\n",
            "0\n",
            "b\"i don't know the stars, or modern chinese teenage music - but i do know a thoroughly entertaining movie when i see one.<br /><br />kung fu dunk is pure hollywood in its values - it's played for laughs, for love, and is a great blend of kung fu and basketball.<br /><br />everybody looks like they had a lot of fun making this - the production values are excellent - and modern china looks glossier than los angeles here.<br /><br />the plot of the abandoned orphan who grows up in a kung fu school only to be kicked out and then discover superstardom as a basketball play (and love and more etc;) is great - this is fresh, fun, and immensely entertaining.<br /><br />with great action and good dialogue this is one simply to enjoy - for all ages - and for our money was one of the best family movies we're seen in a long time.<br /><br />please ignore the negative reviews and give dunk a chance - we were really glad we did - a good sports comedy movie.\"\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the data\n",
        "\n",
        "We'll be using the `keras_nlp.tokenizers.WordPieceTokenizer` layer to tokenize the text. keras_nlp.tokenizers.WordPieceTokenizer takes a WordPiece vocabulary and has functions for tokenizing the text, and detokenizing sequences of tokens.\n",
        "\n",
        "Before we define the tokenizer, we first need to train it on the dataset we have. The WordPiece tokenization algorithm is a subword tokenization algorithm; training it on a corpus gives us a vocabulary of subwords. A subword tokenizer is a compromise between word tokenizers (word tokenizers need very large vocabularies for good coverage of input words), and character tokenizers (characters don't really encode meaning like words do). Luckily, TensorFlow Text makes it very simple to train WordPiece on a corpus.\n",
        "\n",
        "Note: The official implementation of FNet uses the SentencePiece Tokenizer."
      ],
      "metadata": {
        "id": "Ej2N3RN9Zb3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word_piece(ds, vocab_size, reserved_tokens):\n",
        "    bert_vocab_args = dict(\n",
        "        # The target vocabulary size\n",
        "        vocab_size=vocab_size,\n",
        "        # Reserved tokens that must be included in the vocabulary\n",
        "        reserved_tokens=reserved_tokens,\n",
        "        # Arguments for `text.BertTokenizer`\n",
        "        bert_tokenizer_params={\"lower_case\": True},\n",
        "    )\n",
        "\n",
        "    # Extract text samples (remove the labels).\n",
        "    word_piece_ds = ds.unbatch().map(lambda x, y: x)\n",
        "    vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "        word_piece_ds.batch(1000).prefetch(2), **bert_vocab_args\n",
        "    )\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "9A_w44q2adQM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every vocabulary has a few special, reserved tokens. We have two such tokens:\n",
        "\n",
        "* \"[PAD]\" - Padding token. Padding tokens are appended to the input sequence length when the input sequence length is shorter than the maximum sequence length.\n",
        "* \"[UNK]\" - Unknown token."
      ],
      "metadata": {
        "id": "mAjEOSI1fQBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\"]\n",
        "train_sentences = [element[0] for element in train_ds]\n",
        "vocab = train_word_piece(train_ds, VOCAB_SIZE, reserved_tokens)"
      ],
      "metadata": {
        "id": "YrFVOZAgfYpb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see some tokens!"
      ],
      "metadata": {
        "id": "kWGEGaznfjVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens: \", vocab[110:120])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVLIlc57fliG",
        "outputId": "20706162-bdf0-4a11-8e9d-f6d56a06a2d4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['of', 'to', 'is', 'br', 'it', 'in', 'this', 'that', 'was', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's define the tokenizer. We will configure the tokenizer with the the vocabularies trained above. We will define a maximum sequence length so that all sequences are padded to the same length, if the length of the sequence is less than the specified sequence length. Otherwise, the sequence is truncated."
      ],
      "metadata": {
        "id": "zKyDqLqpfqtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=False,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")"
      ],
      "metadata": {
        "id": "yqUcL1LjitjR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and tokenize a sample from our dataset! To verify whether the text has been tokenized correctly, we can also detokenize the list of tokens back to the original text."
      ],
      "metadata": {
        "id": "S3lkI35xiwMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence_ex = train_ds.take(1).get_single_element()[0][0]\n",
        "input_tokens_ex = tokenizer(input_sentence_ex)\n",
        "\n",
        "print(\"Sentence: \", input_sentence_ex)\n",
        "print(\"Tokens: \", input_tokens_ex)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(input_tokens_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRZt69QUiy5W",
        "outputId": "3dc0a7bb-05f2-4652-df89-abecd3869d33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  tf.Tensor(b'i am not so much like love sick as i image. finally the film express sexual relationship of alex, kik, sandu their triangle love were full of intenseness, frustration and jealous, at last, alex waked up and realized that they would not have result and future.ending up was sad.<br /><br />the director tudor giurgiu was in amc theatre on sunday 12:00pm on 08/10/06, with us watched the movie together. after the movie he told the audiences that the purposed to create this film which was to express the sexual relationships of romanian were kind of complicate.<br /><br />on my point of view sexual life is always complicated in everywhere, i don\\'t feel any particular impression and effect from the movie. the love proceeding of alex and kiki, and kiki and her brother sandu were kind of next door neighborhood story.<br /><br />the two main reasons i don\\'t like this movie are, firstly, the film didn\\'t told us how they started to fall in love? sounds like after alex moved into the building which kiki was living, then two girls are fall in love. it doesn\\'t make sense at all. how a girl would fall in love with another girl instead of a man. too much fragments, you need to image and connect those stories by your mind. secondly, the whole film didn\\'t have a scene of alex and kik\\'s sexual intercourse, that \\'s what i was waiting for\\xc2\\x85\\xc2\\x85. however, it still had some parts were deserved to recommend. the \"ear piercing \" part was kind of interesting. alex was willing to suffer the pain of ear piercing to appreciate kik\\'s love. that was a touching scene which gave you a little idea of their love. also, the scene of they were lying in the soccer field, the conversation express their loves were truthful and passionate.', shape=(), dtype=string)\n",
            "Tokens:  tf.Tensor(\n",
            "[   48   353   127   142   178   143   220  1337   119    48  1682    15\n",
            "   557   108   124  3082   991   738   110  2233    13  8677   697    13\n",
            "  5970   780   167  4975   220   173   484   110  1826   526    13  5451\n",
            "   109  4363    13   137   337    13  2233  3778   264   160   109  1902\n",
            "   117   136   164   127   131  1027   109   801    15   385   160   118\n",
            "   732    15    29   113    16    31    29   113    16    31   108   260\n",
            "    59  5359   940 13408  2051  7314   780   118   115 13881  2072   126\n",
            "  3159  1892    27  4927 11691   126    17  2969    16   271    16    17\n",
            "  2818    13   120   288   417   108   122   402    15   204   108   122\n",
            "   128   717   108  1439   117   108  1442   264   111  1125   116   124\n",
            "   166   118   111  3082   108   991  1657   110  9707   173   356   110\n",
            "  2554  7838    15    29   113    16    31    29   113    16    31   126\n",
            "   165   332   110   786   991   221   112   327  2921   115  2860    13\n",
            "    48   193     8    59   340   205  1015  1640   109  1044   141   108\n",
            "   122    15   108   220  9216   206   110  2233   109  8677  2227    13\n",
            "   109  8677  2227   109   145   712  5970   780   173   356   110   477\n",
            "  1340  3946   169    15    29   113    16    31    29   113    16    31\n",
            "   108   211   407  1154    48   193     8    59   143   116   122   129\n",
            "    13  5624    13   108   124   267     8    59   717   288   192   136\n",
            "   784   111   907   115   220    32  1100   143   204  2233  1868   185\n",
            "   108  1421   166  8677  2227   118   690    13   199   211   660   129\n",
            "   907   115   220    15   114   255     8    59   197   384   137   135\n",
            "    15   192    40   347   164   907   115   220   120   269   347   414\n",
            "   110    40   233    15   202   178    45  1523   681  2933    13   125\n",
            "   479   111  1682   109  4325   253   655   138   235   447    15  5028\n",
            "    13   108   328   124   267     8    59   131    40   239   110  2233\n",
            "   109  8677   697     8    58   991  7980 11676    13   117     8    58\n",
            "   152    48   118  1202   121    15   303    13   114   241   172   153\n",
            "   629   173  2091   111   506    15   108     3  6044 14532     3   284\n",
            "   118   356   110   321    15  2233   118  1914   111  2961   108  1724\n",
            "   110  6044 14532   111  1272  8677   697     8    58   220    15   117\n",
            "   118    40  1545   239   166   646   125    40   219   431   110   167\n",
            "   220    15   184    13   108   239   110   136   173  3537   115   108\n",
            "  5726  1968    13   108  2728  3082   167  1458   173 10892   109  4688\n",
            "    15     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0], shape=(512,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b'i am not so much like love sick as i image . finally the film express sexual relationship of alex , kik , sandu their triangle love were full of intenseness , frustration and jealous , at last , alex waked up and realized that they would not have result and future . ending up was sad . < br / > < br / > the director tudor giurgiu was in amc theatre on sunday 12 : 00pm on 08 / 10 / 06 , with us watched the movie together . after the movie he told the audiences that the purposed to create this film which was to express the sexual relationships of romanian were kind of complicate . < br / > < br / > on my point of view sexual life is always complicated in everywhere , i don \\' t feel any particular impression and effect from the movie . the love proceeding of alex and kiki , and kiki and her brother sandu were kind of next door neighborhood story . < br / > < br / > the two main reasons i don \\' t like this movie are , firstly , the film didn \\' t told us how they started to fall in love ? sounds like after alex moved into the building which kiki was living , then two girls are fall in love . it doesn \\' t make sense at all . how a girl would fall in love with another girl instead of a man . too much fragments , you need to image and connect those stories by your mind . secondly , the whole film didn \\' t have a scene of alex and kik \\' s sexual intercourse , that \\' s what i was waiting for . however , it still had some parts were deserved to recommend . the \" ear piercing \" part was kind of interesting . alex was willing to suffer the pain of ear piercing to appreciate kik \\' s love . that was a touching scene which gave you a little idea of their love . also , the scene of they were lying in the soccer field , the conversation express their loves were truthful and passionate . [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Formatting the dataset\n",
        "Next, we'll format our datasets in the form that will be fed to the models. We need to tokenize the text."
      ],
      "metadata": {
        "id": "kUAwqRNci01c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(sentence, label):\n",
        "    sentence = tokenizer(sentence)\n",
        "    return ({\"input_ids\": sentence}, label)\n",
        "\n",
        "\n",
        "def make_dataset(dataset):\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(512).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_ds)\n",
        "val_ds = make_dataset(val_ds)\n",
        "test_ds = make_dataset(test_ds)"
      ],
      "metadata": {
        "id": "p-lyLOx2gLSh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model\n",
        "Let's define the model! We first need an embedding layer, i.e., a layer that maps every token in the input sequence to a vector. This embedding layer can be initialised randomly. We also need a positional embedding layer which encodes the word order in the sequence. The convention is to add, i.e., sum, these two embeddings. KerasNLP has a keras_nlp.layers.TokenAndPositionEmbedding layer which does all of the above steps for us.\n",
        "\n",
        "Our FNet classification model consists of three keras_nlp.layers.FNetEncoder layers with a keras.layers.Dense layer on top.\n",
        "\n",
        "Note: For FNet, masking the padding tokens has a minimal effect on results. In the official implementation, the padding tokens are not masked."
      ],
      "metadata": {
        "id": "a1L6-al-gNW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "fnet_classifier = keras.Model(input_ids, outputs, name=\"fnet_classifier\")"
      ],
      "metadata": {
        "id": "y2Iv9A20gUYA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training our model\n",
        "We'll use accuracy to monitor training progress on the validation data. Let's train our model for 3 epochs."
      ],
      "metadata": {
        "id": "MY9WuaZyh2bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet_classifier.summary()\n",
        "fnet_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fnet_classifier.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYFap9Cnh7_4",
        "outputId": "8736413e-c251-4d06-c919-fb22a557644b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"fnet_classifier\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_ids (InputLayer)      [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, None, 128)        1985536   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " f_net_encoder (FNetEncoder)  (None, None, 128)        132224    \n",
            "                                                                 \n",
            " f_net_encoder_1 (FNetEncode  (None, None, 128)        132224    \n",
            " r)                                                              \n",
            "                                                                 \n",
            " f_net_encoder_2 (FNetEncode  (None, None, 128)        132224    \n",
            " r)                                                              \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 128)              0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,382,337\n",
            "Trainable params: 2,382,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            "260/938 [=======>......................] - ETA: 1:14:38 - loss: -21.9690 - accuracy: 0.1651"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z-o9inhQiF2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}