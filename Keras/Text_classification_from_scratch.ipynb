{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification from scratch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORyshqhRyepjJWzD6y5Nw8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This example shows how to do text classification starting from raw text (as a set of text files on disk). We demonstrate the workflow on the IMDB sentiment classification dataset (unprocessed version). We use the TextVectorization layer for word splitting & indexing."
      ],
      "metadata": {
        "id": "O7N811FFwU0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vNtuCUjdw0eY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "2_HfSLDvw2GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swGuvrlvw4D9",
        "outputId": "f5448fa1-0256-444a-f3dc-c03ba7e89082"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  53.6M      0  0:00:01  0:00:01 --:--:-- 53.6M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The aclImdb/train/pos and aclImdb/train/neg folders contain text files, each of which represents one review (either positive or negative):"
      ],
      "metadata": {
        "id": "14pZrszTw605"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/6248_7.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDwjSlZlxGaG",
        "outputId": "922a031c-625d-4a7f-bb64-075636281995"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being an Austrian myself this has been a straight knock in my face. Fortunately I don't live nowhere near the place where this movie takes place but unfortunately it portrays everything that the rest of Austria hates about Viennese people (or people close to that region). And it is very easy to read that this is exactly the directors intention: to let your head sink into your hands and say \"Oh my god, how can THAT be possible!\". No, not with me, the (in my opinion) totally exaggerated uncensored swinger club scene is not necessary, I watch porn, sure, but in this context I was rather disgusted than put in the right context.<br /><br />This movie tells a story about how misled people who suffer from lack of education or bad company try to survive and live in a world of redundancy and boring horizons. A girl who is treated like a whore by her super-jealous boyfriend (and still keeps coming back), a female teacher who discovers her masochism by putting the life of her super-cruel \"lover\" on the line, an old couple who has an almost mathematical daily cycle (she is the \"official replacement\" of his ex wife), a couple that has just divorced and has the ex husband suffer under the acts of his former wife obviously having a relationship with her masseuse and finally a crazy hitchhiker who asks her drivers the most unusual questions and stretches their nerves by just being super-annoying.<br /><br />After having seen it you feel almost nothing. You're not even shocked, sad, depressed or feel like doing anything... Maybe that's why I gave it 7 points, it made me react in a way I never reacted before. If that's good or bad is up to you!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are only interested in the pos and neg subfolders, so let's delete the rest:"
      ],
      "metadata": {
        "id": "ed9sHUOVxIKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "TxNqIwbSxNm7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the utility tf.keras.preprocessing.text_dataset_from_directory to generate a labeled tf.data.Dataset object from a set of text files on disk filed into class-specific folders.\n",
        "\n",
        "Let's use it to generate the training, validation, and test datasets. The validation and training datasets are generated from two subsets of the train directory, with 20% of samples going to the validation dataset and 80% going to the training dataset.\n",
        "\n",
        "Having a validation dataset in addition to the test dataset is useful for tuning hyperparameters, such as the model architecture, for which the test dataset should not be used.\n",
        "\n",
        "Before putting the model out into the real world however, it should be retrained using all available training data (without creating a validation dataset), so its performance is maximized.\n",
        "\n",
        "When using the validation_split & subset arguments, make sure to either specify a random seed, or to pass shuffle=False, so that the validation & training splits you get have no overlap."
      ],
      "metadata": {
        "id": "97yKnaDCxPJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "RANDOM_SEED = 43\n",
        "\n",
        "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/test',\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
        "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9BweZk3xpZL",
        "outputId": "0833240e-9d52-4c1b-fa85-ed602e12b2d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Number of batches in raw_train_ds: 625\n",
            "Number of batches in raw_val_ds: 157\n",
            "Number of batches in raw_test_ds: 782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's preview a few samples.\n",
        "\n",
        "> `NOTE` : take(1) returns all the samples in the first batch of the dataset. Since our batch size is 32, take(1) will return total 32 examples"
      ],
      "metadata": {
        "id": "-Liza3PjybHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# It's important to take a look at your raw data to ensure your normalization\n",
        "# and tokenization will work as expected. We can do that by taking a few\n",
        "# examples from the training set and looking at them.\n",
        "# This is one of the places where eager execution shines:\n",
        "# we can just evaluate these tensors using .numpy()\n",
        "# instead of needing to evaluate them in a Session/Graph context.\n",
        "\n",
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "    print('*'*50)\n",
        "    print('Total Examples in take(1) of the dataset : ', len(text_batch))\n",
        "    print('*'*50)\n",
        "    for i in range(5):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fH52fHsuzRU6",
        "outputId": "06c0707f-9453-4534-f419-b7566820c88d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total Examples in take(1) of the dataset :  32\n",
            "**************************************************\n",
            "b'Melissa Sagemiller,Wes Bentley,Eliza Dushku and Casey Affleck play young students at Middleton College in the town of Middleton.The four teenagers form two love triangles.One night during an ominous full moon they drive and argue along a slippery and twisting mountain road.Not looking properly they careen into another car and one or more of them are killed.The ghostly nightmare begins...Pretty lousy and politically correct horror flick without gore and nudity.It\\'s obviously influenced by \"Carnival of Souls\".The cinematography is decent,unfortunately there is zero suspense.4 out of 10-just another instantly forgettable teeny-bopper trash.'\n",
            "0\n",
            "b\"There's about 25 years worth of inspiration packed into it. Beginning with existential themes of Blade Runner, as well as the vision of the future - with corporate billboards advertising their products, to the technology of the later Matrix films and Spielberg's A.I., and finally the black and white graphic novel look similar in style of Sin City. The creators have put in a lot of effort in the visual department and the outcome is a well crafted future neo-noir. Add a detective story and you've got an interesting film. I know what it wanted to be, but regardless of the stunning visuals, it wasn't enough to get it to the final destination.\"\n",
            "1\n",
            "b'So Angela has grown up and gotten therapy and an operation to turn her into a real life daughter, rather than the son that she was born, and now holds a job as - wait for it - a camp counselor! How appropriate, right? I know, I love it. Anyway, the first sequel to the Sleepaway Camp franchise obeys all the rules of horror sequels - more blood, more imaginative killings (which aren\\'t imaginative, but still more so than the original), more nudity, a more elaborate plot, and generally worse than the original. <br /><br />It is entertaining in the same way as the original was, in that the characters and wardrobes are so goofy and so authentically 80\\'s that you can\\'t help getting a good laugh. At one point, a guy asks Angela out, and she says \"I\\'ll call you,\" and then quickly walks away. The guy says to himself, \"How is she gonna call me? I don\\'t have a phone!\" and then he sniffs his armpits, wondering what turned her off (it\\'s the hair, dude!!).<br /><br />It is a well-known fact that in 80s slasher movies, the murdered teenagers were more often than not being punished by their killer for some kind of bad behavior, usually for being too promiscuous. When I first started getting into horror movies and saw the Friday the 13th movies for the first time in the mid 90s, I didn\\'t realize this. I learned it in a film class a year or two later and was amazed that their was some method to the madness. I was pretty impressed, not only that the movies were passing on some kind of message, albeit a morbid one, but that there was actually some thought put into it.<br /><br />But not in this movie! At one point just before Angela kills one of her victims, she says \"Let this be a lesson to you. Say no to drugs!\" Real subtle screen writing there, guys. Then again, the dialogue is the most entertaining thing in the movie. Angela (who, by the way, went through all that therapy and those operations and all that trouble to clean up her past and reinvent herself as a normal and well-developed person and then changed her name from Angela to, umm, Angela), says at one point, \"I don\\'t like being the wicked witch of the west, but I know what happens when things get out of control.\" (People start getting killed...by me! HA!)<br /><br />Then later, she demands that one of the counselors, Mare, make an apology, to which the girl replies, \"I\\'d rather die!\" Sorry, Mare, but you really walked into that one...<br /><br />Two years ago I was a camp counselor at a sleepaway camp similar to the one portrayed in this movie (except the camp that I taught at had more than three kids to the 15 or 20 counselors and it also had rules, which the one in the movie doesn\\'t). This made me notice the myriad of discrepancies in the movie from what camp life is really like. <br /><br />That\\'s okay though, you can hardly make a movie like this with a lot of 9 year olds running around, although there were some 10 or 11 year old kids killed in this movie. I hadn\\'t seen that kind of thing much before. <br /><br />Definitely bad taste, even for a cheesy 80s slasher movie....'\n",
            "0\n",
            "b\"I first saw this movie as a pre-teen, about the age when kids start to think through their identity. I was greatly affected by the scene of the man and the children who he raises as his own. The eldest boy has been taunted that his mother is a prostitute and none of his siblings have the same biological father (which Kurosawa makes obvious by having children who look nothing like each other). The man still persuades tho boy that he is their father by the only definition that counts. The man is acclaimed to be father by all of the children but one, who still prefers her brother.<br /><br />Each of the vignettes are likewise compelling for their own stories and conclusions.<br /><br />It's a great film, even if it is not the greatest Kurosawa film.\"\n",
            "1\n",
            "b'Strictly a routine, by-the-numbers western (directed by genre-mainstay Andrew V. McLaglen, so is that any wonder?). Army colonel Brian Keith spars with smarmy bandit Dean Martin, who has just kidnapped the colonel\\'s wife (Honor Blackman, who never found her niche after playing Pussy Galore in \"Goldfinger\"). Fist-fights, shoot-outs, stagecoach robberies and Denver Pyle in a supporting role...in other words, absolutely nothing new or original. Talking in a low monotone throughout, Keith gets to dally with a prostitute (something of a shock after his run on TV\\'s \"Family Affair\"), but otherwise this low-rent material wastes Keith\\'s amiable talents. It\\'s also bad news for Dino, who doesn\\'t seem to notice or care. Hack direction, poor writing and several unfunny attempts at lowball humor. * from ****'\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Cleaning\n",
        "\n",
        "\n",
        "In particular, we remove \"\\<br />\" (Line Break) tags."
      ],
      "metadata": {
        "id": "N0WhIBJP0LTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "# Having looked at our data above, we see that the raw text contains HTML break\n",
        "# tags of the form '<br />'. These tags will not be removed by the default\n",
        "# standardizer (which doesn't strip HTML). Because of this, we will need to\n",
        "# create a custom standardization function.\n",
        "def custom_standardization(input_data):\n",
        "\n",
        "  # Lower input data\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "\n",
        "  # Replace line breaks with space\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "\n",
        "  # Remove punctuations\n",
        "  rem_puncts = tf.strings.regex_replace(\n",
        "      stripped_html, f\"[{re.escape(string.punctuation)}]\", ''\n",
        "      )\n",
        "\n",
        "  return rem_puncts\n",
        "\n",
        "# Testing on a random string\n",
        "custom_standardization('This, is the text string ,<br />??')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCuarqo30NMu",
        "outputId": "22f7f5f6-c626-4fcf-c28a-e4219aa971ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'this is the text string  '>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model constants."
      ],
      "metadata": {
        "id": "8IqAVm2M2Eud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000\n",
        "embedding_dim = 128\n",
        "sequence_length = 500"
      ],
      "metadata": {
        "id": "XA8oNAvx1-5r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have our custom standardization, we can instantiate our text\n",
        "# vectorization layer. We are using this layer to normalize, split, and map\n",
        "# strings to integers, so we set our 'output_mode' to 'int'.\n",
        "# Note that we're using the default split function,\n",
        "# and the custom standardization defined above.\n",
        "# We also set an explicit maximum sequence length, since the CNNs later in our\n",
        "# model won't support ragged sequences.\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# Now that the vocab layer has been created, call `adapt` on a text-only\n",
        "# dataset to create the vocabulary. You don't have to batch, but for very large\n",
        "# datasets this means you're not keeping spare copies of the dataset in memory.\n",
        "\n",
        "# Let's make a text-only dataset (no labels):\n",
        "text_ds = raw_train_ds.map(lambda x, y: x)\n",
        "# Let's call `adapt`:\n",
        "vectorize_layer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "Mz4pGJ3N2Gz0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two options to vectorize the data\n",
        "\n",
        "There are 2 ways we can use our text vectorization layer:"
      ],
      "metadata": {
        "id": "QwH3dWZG2-A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Option 1`: Make it part of the model, so as to obtain a model that processes raw strings, like this:\n",
        "\n",
        "\n",
        ">text_input = tf.keras.Input(shape=(1,), dtype=tf.string, name='text')\\\n",
        "x = vectorize_layer(text_input)\\\n",
        "x = layers.Embedding(max_features + 1, embedding_dim)(x)\\\n",
        "...\n",
        "\n",
        "`Option 2` : Apply it to the text dataset to obtain a dataset of word indices, then feed it into a model that expects integer sequences as inputs.\n",
        "\n",
        "An important difference between the two is that option 2 enables you to do asynchronous CPU processing and buffering of your data when training on GPU. So if you're training the model on GPU, you probably want to go with this option to get the best performance. This is what we will do below.\n",
        "\n",
        "If we were to export our model to production, we'd ship a model that accepts raw strings as input, like in the code snippet for option 1 above. This can be done after training. We do this in the last section."
      ],
      "metadata": {
        "id": "W9xenRG-3EgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), label\n",
        "\n",
        "# Vectorize the data.\n",
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_val_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)\n",
        "\n",
        "# Do async prefetching / buffering of the data for best performance on GPU.\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=10)"
      ],
      "metadata": {
        "id": "djbIZnZw3HjM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see an example"
      ],
      "metadata": {
        "id": "biaHBYDS4CoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    print('*'*50)\n",
        "    print('Total Examples in take(1) of the dataset : ', len(text_batch))\n",
        "    print('*'*50)\n",
        "    for i in range(1):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oFcV0f631sS",
        "outputId": "1e382551-a422-4a75-b16f-eef1dc49aafa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total Examples in take(1) of the dataset :  32\n",
            "**************************************************\n",
            "[    2   287  4313     8     4   795  1935   983     1   371 12998     8\n",
            "     2 18429    13   963     6  4296     2  4313   218   355    16     2\n",
            "   983   638     8     4 12173    44    37    11    13     4  2078    94\n",
            "     6  1022    57     4  2203  2766  2996     1    14  9242     7     4\n",
            "  2235   180   684     3  1101  9644  5929    14     2  1948  4815     3\n",
            " 15087  4348    14     2  8734  5010    23   179   363     2     1     1\n",
            "    23     4     1   588    65   668    23   175     1   188    11     7\n",
            "     2  4897    21     2  1865    37     2    19   146   590   750    47\n",
            "    23     4   165   913    18     2  4313  3733     5   467     7    50\n",
            "  8380    70   617     2   655  1767     7    81     1 12998     8     2\n",
            " 18429     1 13441     1  5989  2086  2547   471  5476  2086  1101  9644\n",
            "  5929     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "0\n"
          ]
        }
      ]
    }
  ]
}