{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Masked Language Modeling with BERT",
      "provenance": [],
      "authorship_tag": "ABX9TyMPutjUVRpQdKB8bwVmze/3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Masked Language Modeling is a fill-in-the-blank task, where a model uses the context words surrounding a mask token to try to predict what the masked word should be.\n",
        "\n",
        "For an input that contains one or more mask tokens, the model will generate the most likely substitution for each.\n",
        "\n",
        "Example:\n",
        "\n",
        "* Input: \"I have watched this [MASK] and it was awesome.\"\n",
        "* Output: \"I have watched this movie and it was awesome.\"\n",
        "Masked language modeling is a great way to train a language model in a self-supervised setting (without human-annotated labels). Such a model can then be fine-tuned to accomplish various supervised NLP tasks.\n",
        "\n",
        "This example explains how to build a BERT model from scratch, train it with the masked language modeling task, and then fine-tune this model on a sentiment classification task.\n",
        "\n",
        "We will use the Keras TextVectorization and MultiHeadAttention layers to create a BERT Transformer-Encoder network architecture.\n",
        "\n",
        "Note: This example should be run with tf-nightly."
      ],
      "metadata": {
        "id": "txxtEYwIIRux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "bkQWNYLyI6JM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 32\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  # used in bert model\n",
        "    FF_DIM = 128  # used in bert model\n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "vEfySSFeJYiA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data\n",
        "We will first download the IMDB data and load into a Pandas dataframe."
      ],
      "metadata": {
        "id": "T0szMnB9Jn9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1fh8yygJrcN",
        "outputId": "46c643c2-7651-4693-85f9-7aee1229a3d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  11.9M      0  0:00:06  0:00:06 --:--:-- 18.6M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)\n",
        "\n",
        "all_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ykq06vgGJtEK",
        "outputId": "1c30bce0-9b7d-4b42-f6ce-834e43e7f054"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  Liked Stanley & Iris very much. Acting was ver...          0\n",
              "1  I loved this film. I first saw it when I was 2...          0\n",
              "2  Who are these \"They\"- the actors? the filmmake...          1\n",
              "3  Trailers of this movie may show scenes of viol...          0\n",
              "4  The first half of this movie is a pure delight...          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-560ff368-1e2f-425b-ac79-1cee83783765\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Liked Stanley &amp; Iris very much. Acting was ver...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I loved this film. I first saw it when I was 2...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who are these \"They\"- the actors? the filmmake...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Trailers of this movie may show scenes of viol...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The first half of this movie is a pure delight...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-560ff368-1e2f-425b-ac79-1cee83783765')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-560ff368-1e2f-425b-ac79-1cee83783765 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-560ff368-1e2f-425b-ac79-1cee83783765');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total records : ', all_data.shape[0])\n",
        "print(all_data['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYXg8fhXKT-s",
        "outputId": "deb7adc5-e374-4899-9a65-ef95c8daf61f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total records :  50000\n",
            "0    25000\n",
            "1    25000\n",
            "Name: sentiment, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation\n",
        "We will use the TextVectorization layer to vectorize the text into integer token ids. It transforms a batch of strings into either a sequence of token indices (one sample = 1D array of integer token indices, in order) or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
        "\n",
        "\n",
        "Below, we define 3 preprocessing functions.\n",
        "\n",
        "1. The get_vectorize_layer function builds the TextVectorization layer.\n",
        "2. The encode function encodes raw text into integer token ids.\n",
        "3. The get_masked_input_and_labels function will mask input token ids. It masks 15% of all input tokens in each sequence at random."
      ],
      "metadata": {
        "id": "1vHKhgpFKo6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )"
      ],
      "metadata": {
        "id": "-zKPsuzzp-tB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer"
      ],
      "metadata": {
        "id": "ML2sKeLXqE77"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "print('Mask Token ID : ', mask_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUXJOuR5qbiu",
        "outputId": "4abde941-f061-4b7c-ee1d-d96c350fadf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask Token ID :  29999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()"
      ],
      "metadata": {
        "id": "50xiUA4iqz0b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try on a sample input text"
      ],
      "metadata": {
        "id": "Q_pdApFdq71n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tststr = ['This is an example text (string) input']\n",
        "\n",
        "print(\"Standardized Version : \", custom_standardization(tststr))\n",
        "print(\"Encoded Version : \", encode(tststr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLD3CPEGrK01",
        "outputId": "fa4dd530-a6ad-40a9-ae6a-3a128c1700d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Version :  tf.Tensor([b'this is an example text string input'], shape=(1,), dtype=string)\n",
            "Encoded Version :  [[   11     7    33   455  3072  3407 12984     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights"
      ],
      "metadata": {
        "id": "JmDxateNrLNS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, on the same example input"
      ],
      "metadata": {
        "id": "TOPbQeKXr1F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_str_encoded = encode(tststr)\n",
        "\n",
        "get_masked_input_and_labels(test_str_encoded)[0] # Notice the element with token id 29999, which is now masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sic3eshor5O7",
        "outputId": "7153476b-c99c-4763-f8ac-263390f77b97"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   11,     7,    33,   455,  3072,  3407, 29999,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "zgFTKIaVsQOP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df.review.values, y_test)\n",
        ").batch(config.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Jp0xZ2aCsr1h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "QC9ZFcy6vgPe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create BERT model (Pretraining Model) for masked language modeling\n",
        "We will create a BERT-like pretraining model architecture using the MultiHeadAttention layer. It will take token ids as inputs (including masked tokens) and it will predict the correct ids for the masked input tokens."
      ],
      "metadata": {
        "id": "1vr_WBTYviXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc"
      ],
      "metadata": {
        "id": "p-iEuOtLBlYP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    '''\n",
        "    BERT module for training Masked Language Model\n",
        "    Query, Key, Value are parameters for attention heads\n",
        "    i is the number of encoder layers\n",
        "    '''\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=f\"encoder_{i}/multiheadattention\",\n",
        "    )(query, key, value)\n",
        "\n",
        "    # Dropout layer\n",
        "    attention_output = layers.Dropout(0.1, name=f\"encoder_{i}/att_dropout\")(\n",
        "        attention_output\n",
        "    )\n",
        "\n",
        "    # Normalization layer\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=f\"encoder_{i}/att_layernormalization\"\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=f\"encoder_{i}/ffn\",\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=f\"encoder_{i}/ffn_dropout\")(\n",
        "        ffn_output\n",
        "    )\n",
        "\n",
        "    # Final Normalization layer\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=f\"encoder_{i}/ffn_layernormalization\"\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output"
      ],
      "metadata": {
        "id": "s4ZZE6HtBv7f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "5kIgC70iDDtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")"
      ],
      "metadata": {
        "id": "bhdeF4tiDGri"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    '''\n",
        "    Child class of tf.keras.Model\n",
        "    '''\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]"
      ],
      "metadata": {
        "id": "RNOD822vDJ1p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masked_language_bert_model():\n",
        "\n",
        "    # Define Input Layer\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    # Define Word Embedding Layer and pass inputs through it\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "\n",
        "    # Define positional embedding layer\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "\n",
        "    # Update Word Embeddings with positional embeddings\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    # Generate Encoder Output for number of encoder layers\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    # Pass encoder output through a dense layer\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "\n",
        "    # pass input and output to MaskedLanguageModel class\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    # Define Optimizer and compile the model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "\n",
        "    return mlm_model"
      ],
      "metadata": {
        "id": "hgnPHkuCDtA-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token2ID and ID2Token"
      ],
      "metadata": {
        "id": "U1i6j6rqFLSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}"
      ],
      "metadata": {
        "id": "b_pRGGvoFOvH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V--h6krmFQ9m",
        "outputId": "da7bf665-499b-49ef-de67-08540bbbb799"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " word_embedding (Embedding)     (None, 256, 128)     3840000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 256, 128)    0           ['word_embedding[0][0]']         \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " encoder_0/multiheadattention (  (None, 256, 128)    66048       ['tf.__operators__.add[0][0]',   \n",
            " MultiHeadAttention)                                              'tf.__operators__.add[0][0]',   \n",
            "                                                                  'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " encoder_0/att_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/multiheadattention[0]\n",
            " )                                                               [0]']                            \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 256, 128)    0           ['tf.__operators__.add[0][0]',   \n",
            " mbda)                                                            'encoder_0/att_dropout[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_0/att_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_1[0][0]'] \n",
            " on (LayerNormalization)                                                                          \n",
            "                                                                                                  \n",
            " encoder_0/ffn (Sequential)     (None, 256, 128)     33024       ['encoder_0/att_layernormalizatio\n",
            "                                                                 n[0][0]']                        \n",
            "                                                                                                  \n",
            " encoder_0/ffn_dropout (Dropout  (None, 256, 128)    0           ['encoder_0/ffn[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 256, 128)    0           ['encoder_0/att_layernormalizatio\n",
            " mbda)                                                           n[0][0]',                        \n",
            "                                                                  'encoder_0/ffn_dropout[0][0]']  \n",
            "                                                                                                  \n",
            " encoder_0/ffn_layernormalizati  (None, 256, 128)    256         ['tf.__operators__.add_2[0][0]'] \n",
            " on (LayerNormalization)                                                                          \n",
            "                                                                                                  \n",
            " mlm_cls (Dense)                (None, 256, 30000)   3870000     ['encoder_0/ffn_layernormalizatio\n",
            "                                                                 n[0][0]']                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7,809,584\n",
            "Trainable params: 7,809,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5PwtuJYKGRF",
        "outputId": "062118de-b3cb-4006-87fb-c9b2ef9ecb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 629/1563 [===========>..................] - ETA: 1:05:09 - loss: 7.1934"
          ]
        }
      ]
    }
  ]
}