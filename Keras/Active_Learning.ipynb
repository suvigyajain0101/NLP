{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Active Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPV3kSeNjt67WZXQLsbL9oU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Active Learning\n",
        "\n",
        "\n",
        "With the growth of data-centric Machine Learning, Active Learning has grown in popularity amongst businesses and researchers. Active Learning seeks to progressively train ML models so that the resultant model requires lesser amount of training data to achieve competitive scores.\n",
        "\n",
        "The structure of an Active Learning pipeline involves a classifier and an oracle. The oracle is an annotator that cleans, selects, labels the data, and feeds it to the model when required. The oracle is a trained individual or a group of individuals that ensure consistency in labeling of new data.\n",
        "\n",
        "The process starts with annotating a small subset of the full dataset and training an initial model. The best model checkpoint is saved and then tested on a balanced test set. The test set must be carefully sampled because the full training process will be dependent on it. Once we have the initial evaluation scores, the oracle is tasked with labeling more samples; the number of data points to be sampled is usually determined by the business requirements. After that, the newly sampled data is added to the training set, and the training procedure repeats. This cycle continues until either an acceptable score is reached or some other business metric is met.\n",
        "\n",
        "This tutorial provides a basic demonstration of how Active Learning works by demonstrating a ratio-based (least confidence) sampling strategy that results in lower overall false positive and negative rates when compared to a model trained on the entire dataset. This sampling falls under the domain of uncertanity sampling, in which new datasets are sampled based on the uncertanity that the model outputs for the corresponding label. In our example, we compare our model's false positive and false negative rates and annotate the new data based on their ratio.\n",
        "\n",
        "Some other sampling techniques include:\n",
        "\n",
        "1. Committee sampling: Using multiple models to vote for the best data points to be sampled\n",
        "2. Entropy reduction: Sampling according to an entropy threshold, selecting more of the samples that produce the highest entropy score.\n",
        "3. Minimum margin based sampling: Selects data points closest to the decision boundary"
      ],
      "metadata": {
        "id": "iwWgfkedaC1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "\n",
        "tfds.disable_progress_bar()"
      ],
      "metadata": {
        "id": "uOY69YD_aZl9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and preprocessing the data\n",
        "\n",
        "We will be using the IMDB reviews dataset for our experiments. This dataset has 50,000 reviews in total, including training and testing splits. We will merge these splits and sample our own, balanced training, validation and testing sets."
      ],
      "metadata": {
        "id": "wAJA6Sztab3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tfds.load(\n",
        "    \"imdb_reviews\",\n",
        "    split=\"train + test\",\n",
        "    as_supervised=True,\n",
        "    batch_size=-1,\n",
        "    shuffle_files=False,\n",
        ")\n",
        "reviews, labels = tfds.as_numpy(dataset)\n",
        "\n",
        "print(\"Total examples:\", reviews.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L98KityUatBF",
        "outputId": "94cfec93-53e8-42b3-d9b2-c31f392916ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n",
            "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "Total examples: 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Active learning starts with labeling a subset of data. For the ratio sampling technique that we will be using, we will need well-balanced training, validation and testing splits."
      ],
      "metadata": {
        "id": "ewHk_XOka03O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_split = 2500\n",
        "test_split = 2500\n",
        "train_split = 7500"
      ],
      "metadata": {
        "id": "WgsaDyPda4sN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating the negative and positive samples for manual stratification\n",
        "x_positives, y_positives = reviews[labels == 1], labels[labels == 1]\n",
        "x_negatives, y_negatives = reviews[labels == 0], labels[labels == 0]"
      ],
      "metadata": {
        "id": "ttVTqLG8a63f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test, y_test = (\n",
        "    tf.concat(\n",
        "        (\n",
        "            x_positives[val_split : val_split + test_split],\n",
        "            x_negatives[val_split : val_split + test_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        "    tf.concat(\n",
        "        (\n",
        "            y_positives[val_split : val_split + test_split],\n",
        "            y_negatives[val_split : val_split + test_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        ")\n",
        "x_train, y_train = (\n",
        "    tf.concat(\n",
        "        (\n",
        "            x_positives[val_split + test_split : val_split + test_split + train_split],\n",
        "            x_negatives[val_split + test_split : val_split + test_split + train_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        "    tf.concat(\n",
        "        (\n",
        "            y_positives[val_split + test_split : val_split + test_split + train_split],\n",
        "            y_negatives[val_split + test_split : val_split + test_split + train_split],\n",
        "        ),\n",
        "        0,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "oMxXMfA7a9u-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remaining pool of samples are stored separately. These are only labeled as and when required\n",
        "x_pool_positives, y_pool_positives = (\n",
        "    x_positives[val_split + test_split + train_split :],\n",
        "    y_positives[val_split + test_split + train_split :],\n",
        ")\n",
        "x_pool_negatives, y_pool_negatives = (\n",
        "    x_negatives[val_split + test_split + train_split :],\n",
        "    y_negatives[val_split + test_split + train_split :],\n",
        ")"
      ],
      "metadata": {
        "id": "LIzKrZRHa_c0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating TF Datasets for faster prefetching and parallelization\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "pool_negatives = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_pool_negatives, y_pool_negatives)\n",
        ")\n",
        "pool_positives = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_pool_positives, y_pool_positives)\n",
        ")\n",
        "\n",
        "print(f\"Initial training set size: {len(train_dataset)}\")\n",
        "print(f\"Validation set size: {len(val_dataset)}\")\n",
        "print(f\"Testing set size: {len(test_dataset)}\")\n",
        "print(f\"Unlabeled negative pool: {len(pool_negatives)}\")\n",
        "print(f\"Unlabeled positive pool: {len(pool_positives)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aazd_2tZbkFh",
        "outputId": "6d538b9e-9645-4cad-dfa7-87349aa2cd2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial training set size: 15000\n",
            "Validation set size: 5000\n",
            "Testing set size: 5000\n",
            "Unlabeled negative pool: 12500\n",
            "Unlabeled positive pool: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fitting the TextVectorization layer\n",
        "Since we are working with text data, we will need to encode the text strings as vectors which would then be passed through an Embedding layer. To make this tokenization process faster, we use the map() function with its parallelization functionality."
      ],
      "metadata": {
        "id": "68oiNw7bbpW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom standardization function - \n",
        "# lower the text, remove line breaks, remove punctuations\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Create a vectorization layer, with max_features=3000, \n",
        "# custom standardizer and output seq len = 150\n",
        "vectorizer = layers.TextVectorization(\n",
        "    3000, standardize=custom_standardization, output_sequence_length=150\n",
        ")\n",
        "\n",
        "# Adapting on the training dataset\n",
        "# This means that the vectorizer will use the vocab from training dataset\n",
        "vectorizer.adapt(\n",
        "    train_dataset.map(lambda x, y: x, num_parallel_calls=tf.data.AUTOTUNE).batch(256)\n",
        ")\n",
        "\n",
        "# Create the function to map\n",
        "def vectorize_text(text, label):\n",
        "    text = vectorizer(text)\n",
        "    return text, label\n",
        "\n",
        "# Map the vectorizer function on train, test and val datasets\n",
        "# Regarding num_parallel_calls : A tf.int64 scalar tf.Tensor, \n",
        "# representing the number of batches to compute asynchronously in parallel. \n",
        "# If not specified, batches will be computed sequentially. \n",
        "# If the value tf.data.AUTOTUNE is used, then the number of parallel calls is\n",
        "# set dynamically based on available resources\n",
        "train_dataset = train_dataset.map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = val_dataset.batch(256).map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "test_dataset = test_dataset.batch(256).map(\n",
        "    vectorize_text, num_parallel_calls=tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Map the vectorizer function on unlabelled positives and negatives\n",
        "pool_negatives = pool_negatives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "pool_positives = pool_positives.map(vectorize_text, num_parallel_calls=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "EMmHWSTHb-eY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Helper Functions"
      ],
      "metadata": {
        "id": "NJo7l1KYdK5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for merging new history objects with older ones\n",
        "def append_history(losses, val_losses, accuracy, val_accuracy, history):\n",
        "    losses = losses + history.history[\"loss\"]\n",
        "    val_losses = val_losses + history.history[\"val_loss\"]\n",
        "    accuracy = accuracy + history.history[\"binary_accuracy\"]\n",
        "    val_accuracy = val_accuracy + history.history[\"val_binary_accuracy\"]\n",
        "    return losses, val_losses, accuracy, val_accuracy\n",
        "\n",
        "# Plotter function\n",
        "def plot_history(losses, val_losses, accuracies, val_accuracies):\n",
        "    plt.plot(losses)\n",
        "    plt.plot(val_losses)\n",
        "    plt.legend([\"train_loss\", \"val_loss\"])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(accuracies)\n",
        "    plt.plot(val_accuracies)\n",
        "    plt.legend([\"train_accuracy\", \"val_accuracy\"])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "qe0UVytHdrdz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2rh00aMjds8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}