{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPviXMXavvBD8Qehn9dlq/B"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The notebook refers the work in this [Paper](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
      ],
      "metadata": {
        "id": "YBGYzQc6F-1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "5k1ekXFeEMWf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1LsWxRzCRPT",
        "outputId": "751dc67c-394d-4551-bd06-dd155f64b094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 21:48:17--  https://raw.githubusercontent.com/suvigyajain0101/NLP/llms/LLMs/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-06 21:48:17 (8.31 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/suvigyajain0101/NLP/llms/LLMs/names.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt').read().splitlines()\n",
        "words[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7SpoF0NCWhj",
        "outputId": "97b392a7-1a33-4752-a8fd-2736cbb72a2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma',\n",
              " 'olivia',\n",
              " 'ava',\n",
              " 'isabella',\n",
              " 'sophia',\n",
              " 'charlotte',\n",
              " 'mia',\n",
              " 'amelia',\n",
              " 'harper',\n",
              " 'evelyn']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_GFD1NPFNgv",
        "outputId": "fda9963d-64cf-4876-a325-f73a715f6124"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab of characters and mapping to and from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "assert len(chars) == 26\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "\n",
        "print(stoi)\n",
        "print(itos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHRK3o2kCY47",
        "outputId": "2687aedf-0f8e-4ebd-aa57-29e320314c65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation"
      ],
      "metadata": {
        "id": "nd5gE67FFy_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Length - How many characters are used to predict the next one\n",
        "block_size = 3\n",
        "\n",
        "X, Y = [], []\n",
        "for w in words[0:5]:\n",
        "\n",
        "  print(w)\n",
        "  context = [0] * block_size\n",
        "  for ch in w + '.':\n",
        "    ix = stoi[ch]\n",
        "    X.append(context)\n",
        "    Y.append(ix)\n",
        "\n",
        "    print(''.join(itos[i] for i in context), '------>', itos[ix])\n",
        "\n",
        "    context = context[1:] + [ix]\n",
        "\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ0U3X5tGPdP",
        "outputId": "c3e16354-3dea-47af-8285-85c09aac4ce3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emma\n",
            "... ------> e\n",
            "..e ------> m\n",
            ".em ------> m\n",
            "emm ------> a\n",
            "mma ------> .\n",
            "olivia\n",
            "... ------> o\n",
            "..o ------> l\n",
            ".ol ------> i\n",
            "oli ------> v\n",
            "liv ------> i\n",
            "ivi ------> a\n",
            "via ------> .\n",
            "ava\n",
            "... ------> a\n",
            "..a ------> v\n",
            ".av ------> a\n",
            "ava ------> .\n",
            "isabella\n",
            "... ------> i\n",
            "..i ------> s\n",
            ".is ------> a\n",
            "isa ------> b\n",
            "sab ------> e\n",
            "abe ------> l\n",
            "bel ------> l\n",
            "ell ------> a\n",
            "lla ------> .\n",
            "sophia\n",
            "... ------> s\n",
            "..s ------> o\n",
            ".so ------> p\n",
            "sop ------> h\n",
            "oph ------> i\n",
            "phi ------> a\n",
            "hia ------> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example data, label in the dataset\n",
        "print(X[1], Y[1])\n",
        "\n",
        "X.shape, X.dtype, Y.shape, Y.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOgT4RWYJd8T",
        "outputId": "ce5f91e2-77f4-4c91-9c9c-ff47eb073809"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 0, 5]) tensor(13)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding Matrix\n",
        "\n",
        "The awesomeness of Pytorch indexing!"
      ],
      "metadata": {
        "id": "bGVRWCuBKW0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's assume we want to embed chars to 2 axes\n",
        "C = torch.randn((27, 2))"
      ],
      "metadata": {
        "id": "Nx_j7I0oL5Lm"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in order to get the element from C, we can use 2 approaches - \n",
        "# 1. Index the tensor, just like python list\n",
        "# 2. Matrix multiplication -> remember, one-hot matrix when multiplied with another matrix gives the row of interest\n",
        "# Obviously, it's much faster to just use indexing, and we'll park the idea of matrix multiplication for now\n",
        "C[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZrWwhiMDT6",
        "outputId": "fae01e8f-61d7-4671-a1fd-9d63e4b7c2f0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4540, 0.2732])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# But instead of 1 int, we need to index a matrix of shape (27, 2)\n",
        "# Turns out, Pytorch can take care of this\n",
        "C[X].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcXfA7TTMfpz",
        "outputId": "be8bdbb3-222d-4bd2-e3e5-33dbfb67d657"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 elements of the embedding matrix for every element (32, 3) of the input"
      ],
      "metadata": {
        "id": "y1oJNKooMyDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can verify\n",
        "\n",
        "test_output = C[X]\n",
        "\n",
        "test_row = 3\n",
        "print('input row: ', X[test_row])\n",
        "print('embedding matrix row: ', C[X[test_row]])\n",
        "print('Indexed row: ', test_output[test_row])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hsgz_aCM3z7",
        "outputId": "67ad9ac8-207a-429e-cb81-b4e980555b18"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input row:  tensor([ 5, 13, 13])\n",
            "embedding matrix row:  tensor([[ 0.4540,  0.2732],\n",
            "        [ 1.8485, -0.3960],\n",
            "        [ 1.8485, -0.3960]])\n",
            "Indexed row:  tensor([[ 0.4540,  0.2732],\n",
            "        [ 1.8485, -0.3960],\n",
            "        [ 1.8485, -0.3960]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matches perfectly!"
      ],
      "metadata": {
        "id": "gHl0z7ucNaT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emb = C[X]\n",
        "emb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_5deiHANi8n",
        "outputId": "2b3c4296-d8c7-45ec-e8c7-3480714d624f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hidden layer"
      ],
      "metadata": {
        "id": "P6t1QN2PNp9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = torch.randn((6, 100))\n",
        "# 6 because for every context block of 3 chars, we have 2-dim embedding\n",
        "# 100 is a hyper-parameter\n",
        "\n",
        "b1 = torch.randn(100)"
      ],
      "metadata": {
        "id": "RqBn_OlGNytU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to do emb @ W1 + b, we need to re-arrange the matrices\n",
        "# For now, these are not multipliable\n",
        "# There are multiple ways in Pytorch to do this"
      ],
      "metadata": {
        "id": "0_dWZrBsQ8lR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. using cat\n",
        "torch.cat([emb[:, 0, :] , emb[:, 1, :] , emb[:, 2, :]], 1).shape\n",
        "\n",
        "# Need to update code everytime when block_size is changes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDxl_kyhQ97M",
        "outputId": "fb7e5954-350d-4b33-d73a-5255d230fb83"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. using unbind\n",
        "torch.cat(torch.unbind(emb, 1), 1).shape\n",
        "\n",
        "# Costly, specially memory wise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rExQ5LtpRxKr",
        "outputId": "a1bdaf78-bd19-4c19-9cb0-b2606fb731cc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. re-arranging using torch internals - update the view of the tensor\n",
        "emb.view(32, 6).shape\n",
        "\n",
        "# Fastest and cheapest method of doing this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oNwd6F5RxE7",
        "outputId": "7e46d772-2472-472e-be44-d72703f99235"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if the results are correct\n",
        "# torch.all checks condition on all element of the tensor\n",
        "print(torch.all((torch.cat([emb[:, 0, :] , emb[:, 1, :] , emb[:, 2, :]], 1) == torch.cat(torch.unbind(emb, 1), 1)) == True))\n",
        "print(torch.all((torch.cat([emb[:, 0, :] , emb[:, 1, :] , emb[:, 2, :]], 1) == emb.view(32, 6)) == True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgyX-zrdSixG",
        "outputId": "fab7db0a-00a8-4734-9c0c-7f4ec0e918e0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting back to the hidden layer"
      ],
      "metadata": {
        "id": "AZOLX0oqTvcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h0 = emb.view(-1, 6) @ W1 + b1 # If -1, pytorch infers the dims\n",
        "h = torch.tanh(h0)\n",
        "\n",
        "h.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOKNHP-AT0nZ",
        "outputId": "6fc840ae-b6ef-456a-a645-72378bc19c8d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax layer and Loss"
      ],
      "metadata": {
        "id": "4HV30MiLUjyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = torch.randn((100, 27))\n",
        "b2 = torch.randn(27)"
      ],
      "metadata": {
        "id": "TQKsgpq-U__5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = h @ W2 + b2\n",
        "logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh9BJCuPVG9l",
        "outputId": "11b0aba0-e1a4-4fba-f5dc-cd81b06d8940"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdims=True)\n",
        "\n",
        "# Notice the indexing of probs - we need to extract the probs at that particular Y index\n",
        "loss = -probs[torch.arange(32),Y].log().mean()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XBiyehbVO9q",
        "outputId": "e8ac404c-ccac-46fc-ef00-3879ee84a779"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.3551)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The above is just for educational purpose and is rarely used in practice\n",
        "loss = F.cross_entropy(logits, Y)\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SKG6QICYb6W",
        "outputId": "8ee9c82b-8b7b-4c33-eac3-89698d777f59"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(17.3551)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few reasons\n",
        "1. Efficient under-the-hood implementation. Pytorch won't create so many additional tensors and save on memory. Also, both forward and backward passes are much more efficient\n",
        "2. Exp overflow - When input increases in +ve direction, exp() tends to reach inf. Internally, Pytorch subtracts max of the input and avoids inf (since, [10,15,20,25] will have same output as [9,14,19,24])"
      ],
      "metadata": {
        "id": "jT6CPLLOYpTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop\n",
        "\n",
        "Putting it all together"
      ],
      "metadata": {
        "id": "tgOKBFCjV7GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsfJRhDMV8YL",
        "outputId": "dfe43bf2-2785-4ad7-f33f-197fa256cfba"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 3]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(10)\n",
        "C = torch.randn((27, 2), generator=g)\n",
        "W1 = torch.randn((6, 100), generator=g)\n",
        "b1 = torch.randn(100, generator=g)\n",
        "W2 = torch.randn((100, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "wQQHi3BzWpJt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total trainable parameters: ', sum(p.nelement() for p in parameters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzxxUKKzdv0p",
        "outputId": "330308c1-eb9d-4e7c-d69f-5bca45a1b16c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters:  3481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure all the parameters support gradient and updates\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n"
      ],
      "metadata": {
        "id": "bO1kWJ_2e8nL"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(10):\n",
        "  # Forward Pass\n",
        "  emb = C[X]\n",
        "  h0 = emb.view(-1, 6) @ W1 + b1\n",
        "  h = torch.tanh(h0)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Y)\n",
        "  print(f'Epoch {k} -> Loss = {loss.item()} ')\n",
        "\n",
        "  # Backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # Gradient Update\n",
        "  for p in parameters:\n",
        "    p.data += -0.1 * p.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRq19nqIeD3U",
        "outputId": "c3a0d13b-4a2b-4834-a3b3-3ddf23fa4b82"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 -> Loss = 19.009180068969727 \n",
            "Epoch 1 -> Loss = 15.267385482788086 \n",
            "Epoch 2 -> Loss = 12.383593559265137 \n",
            "Epoch 3 -> Loss = 10.268889427185059 \n",
            "Epoch 4 -> Loss = 8.576563835144043 \n",
            "Epoch 5 -> Loss = 7.1071906089782715 \n",
            "Epoch 6 -> Loss = 5.814657688140869 \n",
            "Epoch 7 -> Loss = 4.639155387878418 \n",
            "Epoch 8 -> Loss = 3.6986703872680664 \n",
            "Epoch 9 -> Loss = 3.0502424240112305 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### On Complete dataset "
      ],
      "metadata": {
        "id": "cJbchfCdgMQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Context Length - How many characters are used to predict the next one\n",
        "block_size = 3\n",
        "\n",
        "X, Y = [], []\n",
        "for w in words:\n",
        "  context = [0] * block_size\n",
        "  for ch in w + '.':\n",
        "    ix = stoi[ch]\n",
        "    X.append(context)\n",
        "    Y.append(ix)\n",
        "\n",
        "    context = context[1:] + [ix]\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)\n",
        "\n",
        "X.shape, Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJaYVi5WgVmq",
        "outputId": "568cad9f-dc7d-4a6f-f9b3-3442052a773e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([228146, 3]), torch.Size([228146]))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(10)\n",
        "C = torch.randn((27, 2), generator=g)\n",
        "W1 = torch.randn((6, 100), generator=g)\n",
        "b1 = torch.randn(100, generator=g)\n",
        "W2 = torch.randn((100, 27), generator=g)\n",
        "b2 = torch.randn(27, generator=g)\n",
        "parameters = [C, W1, b1, W2, b2]"
      ],
      "metadata": {
        "id": "-R9ANY9Fgc7Z"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total trainable parameters: ', sum(p.nelement() for p in parameters))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-U-uB4ZggQP",
        "outputId": "e2c3b0af-b15e-4e35-8b2c-f33dead0dc5a"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters:  3481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure all the parameters support gradient and updates\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "DhURGNo1gh_j"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(1000):\n",
        "  # Forward Pass\n",
        "  emb = C[X]\n",
        "  h0 = emb.view(-1, 6) @ W1 + b1\n",
        "  h = torch.tanh(h0)\n",
        "  logits = h @ W2 + b2\n",
        "  loss = F.cross_entropy(logits, Y)\n",
        "\n",
        "  if k%100 == 0:\n",
        "    print(f'Epoch {k} -> Loss = {loss.item()} ')\n",
        "\n",
        "  # Backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # Gradient Update\n",
        "  for p in parameters:\n",
        "    p.data += -0.1 * p.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fAUbHLCgje2",
        "outputId": "cfef66a4-8d5e-4946-c32c-885979aec280"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 -> Loss = 18.250364303588867 \n",
            "Epoch 100 -> Loss = 3.490938901901245 \n",
            "Epoch 200 -> Loss = 2.9825150966644287 \n",
            "Epoch 300 -> Loss = 2.7816600799560547 \n",
            "Epoch 400 -> Loss = 2.689009428024292 \n",
            "Epoch 500 -> Loss = 2.6442503929138184 \n",
            "Epoch 600 -> Loss = 2.613981008529663 \n",
            "Epoch 700 -> Loss = 2.590808153152466 \n",
            "Epoch 800 -> Loss = 2.573031187057495 \n",
            "Epoch 900 -> Loss = 2.55879545211792 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "ujXrOK1lg8Z5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}