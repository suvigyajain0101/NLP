{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_NLP_in_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFfiH3gXERifICOA7iXJeN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46QLXRNPdk5f"
      },
      "source": [
        "# But, what is NLP?\n",
        "\n",
        "NLP is a branch of AI, that deals with the interaction between computers and humans using the natural language. Most NLP techniques rely on Machine Learning to make sense of natural language. \n",
        "The ultimate objective of NLP is to read, decipher, understand and make sense of human languages\n",
        "\n",
        "Typical NLP use cases - \n",
        "* Voice assistants, Ok Google, Siri, Cortana\n",
        "* Language Translation, Google translate, Bing\n",
        "* Gmail's spam detection filter\n",
        "* MS Word's grammatical mistake feature\n",
        "\n",
        "There are 2 broad approaches of handling NLP problems - Syntactic and Semantic Analysis\n",
        "\n",
        "1. Syntax - Syntax in a code means the rules that need to followed for that particular programming language. Syntax in a language means the grammatical rules that need to be followed.\n",
        "      Like POS tagging, lemmatization, stemming etc\n",
        "\n",
        "2. Semantic - Semantic analytics dive deeper into the language to understand meaning that is conveyed by the text and sentence structure.\n",
        "      Like NERs, NLU etc\n",
        "\n",
        "\n",
        "Above is a short summary of this [Article](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
        "\n",
        "From machine learning perspective, NLP is about being as creative as we can get with converting our text to numbers. Why is that? Because ML models don't take text as input. In this notebook, we will use the following approach to break an NLP problem and derive insights from it - \n",
        "1. Get the text\n",
        "2. Convert it into numbers\n",
        "3. Modelling\n",
        "4. Predictions\n",
        "\n",
        "\n",
        "We will start with a baseline TF-IDF model and build on it using DNN, LSTM, GRU, Conv and Transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbiyDMHrn91b"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "\n",
        "# Let's define Universal Random State\n",
        "random_state = 42\n",
        "\n",
        "# For TensorBoard, let's define log storage directory\n",
        "SAVE_DIR = 'model_logs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJ4nlQDnNbS"
      },
      "source": [
        "#### The below code block can be used to download data from kaggle directly into Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lC1oC9WkSpS"
      },
      "source": [
        "## Data - We will use Kaggle's disaster tweets dataset\n",
        "\n",
        "# Natural Language Processing with Disaster Tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyYirAqlOsE",
        "outputId": "177cf989-6ea4-4056-df62-a6d195adae2c"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/e7/3bac01547d2ed3d308ac92a0878fbdb0ed0f3d41fb1906c319ccbba1bfbc/kaggle-1.5.12.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 12.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 15.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-cp37-none-any.whl size=73053 sha256=88e95e4062e423d623585629226e2785b8937d7d9930b1b43371d22f593dceb0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6a/26/d30b7499ff85a4a4593377a87ecf55f7d08af42f0de9b60303\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "DVChbXkKlqgf",
        "outputId": "5768ced2-974a-4aab-8ddf-6ebb11a53ec0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-70885d63-8996-48fe-b477-79616ec67950\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-70885d63-8996-48fe-b477-79616ec67950\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"suvigyajain\",\"key\":\"82b0c72e48196f9a45aa86528a87c74d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg-uesRWmLhz"
      },
      "source": [
        "! mkdir ~/.kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6zDaCmdmMAZ"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExGeZ-lLmMHC"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL_JCyCimMMs",
        "outputId": "159d8742-00da-420e-a3ad-34e6cc496290"
      },
      "source": [
        "! kaggle competitions download -c 'nlp-getting-started'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nlp-getting-started.zip to /content\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 110MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIO6NuimMRz"
      },
      "source": [
        "! mkdir input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OMwU_AZmMWa",
        "outputId": "8606052f-6175-4b4d-b4f2-bd84bf5b4339"
      },
      "source": [
        "! unzip nlp-getting-started.zip -d input_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nlp-getting-started.zip\n",
            "  inflating: input_data/sample_submission.csv  \n",
            "  inflating: input_data/test.csv     \n",
            "  inflating: input_data/train.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOhT97ynZh6"
      },
      "source": [
        "### Let's deep dive in the data\n",
        "\n",
        "Kaggle created 3 files for us - train, test and sample-submission\n",
        "We will read the files, do some EDA and then move forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0V2ICTn0gz",
        "outputId": "c4ca4c87-2549-4cc3-ddf4-63059bc878f3"
      },
      "source": [
        "train_df = pd.read_csv('input_data/train.csv')\n",
        "test_df = pd.read_csv('input_data/test.csv')\n",
        "\n",
        "print(\"Train Data Size : \", len(train_df))\n",
        "print(\"Test Data Size : \", len(test_df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Size :  7613\n",
            "Test Data Size :  3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IrVwgNxtojDI",
        "outputId": "451096a6-3ea4-4291-9e64-4e8c28fb8d26"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qPct8kRepG0h",
        "outputId": "eac95cd0-2100-4b2b-ac02-efd80b6b28ea"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjbBKJispQFh"
      },
      "source": [
        "# Data desc on Kaggle mentions that the location has this distribution - \n",
        "                  # USA -> 1%\n",
        "                  # RoW -> 65%\n",
        "                  # Null-> 33%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ze_se3oeqToS",
        "outputId": "25ab2260-ee59-4420-ffdd-1b14e3d20916"
      },
      "source": [
        "# What about keywords?\n",
        "train_df[train_df['keyword'].notnull()].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n",
              "32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n",
              "33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n",
              "34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n",
              "35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB3RqRCMq0wy",
        "outputId": "782a984d-e109-4e69-d588-0dfe116bbb7a"
      },
      "source": [
        "# Are key words always part of the text? If so, then can we just ignore those?\n",
        "train_df[train_df['keyword'].notnull()].apply(lambda x: x.keyword in x.text, axis=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31       True\n",
              "32      False\n",
              "33       True\n",
              "34       True\n",
              "35      False\n",
              "        ...  \n",
              "7578     True\n",
              "7579     True\n",
              "7580     True\n",
              "7581     True\n",
              "7582     True\n",
              "Length: 7552, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7DOGJyNsGNl"
      },
      "source": [
        "# Not always true!! Let's see in the later stages if we can somehow use this added information"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPWzc21MzTog",
        "outputId": "ad206f2c-98bd-4d50-d9d7-e7f4c22fb955"
      },
      "source": [
        "train_df['target'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNJp_-fEzYfC"
      },
      "source": [
        "Another Binary Classification Problem. Fairly balanced target (60-40)\n",
        "\n",
        "  1 - Disaster Related Tweet\n",
        "  \n",
        "  2 - Not related to Disaster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYR13Mz0ze0t",
        "outputId": "5c925162-9c67-452f-d172-7fe111b5ae36"
      },
      "source": [
        "# Let's print out few 1s and 0s\n",
        "\n",
        "positive_tweet_sample = train_df[train_df['target'] == 1]['text'].head().tolist()\n",
        "negative_tweet_sample = train_df[train_df['target'] == 0]['text'].head().tolist()\n",
        "\n",
        "\n",
        "print(\"Disaster Related Tweets : \", '\\n')\n",
        "for i in range(5):\n",
        "  print('\\t', positive_tweet_sample[i])\n",
        "  print('\\t', '-'*20)\n",
        "print('\\n')\n",
        "print(\"Non-Disaster Related Tweets : \", '\\n')\n",
        "for i in range(5):\n",
        "  print('\\t', negative_tweet_sample[i])\n",
        "  print('\\t', '-'*20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disaster Related Tweets :  \n",
            "\n",
            "\t Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "\t --------------------\n",
            "\t Forest fire near La Ronge Sask. Canada\n",
            "\t --------------------\n",
            "\t All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "\t --------------------\n",
            "\t 13,000 people receive #wildfires evacuation orders in California \n",
            "\t --------------------\n",
            "\t Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
            "\t --------------------\n",
            "\n",
            "\n",
            "Non-Disaster Related Tweets :  \n",
            "\n",
            "\t What's up man?\n",
            "\t --------------------\n",
            "\t I love fruits\n",
            "\t --------------------\n",
            "\t Summer is lovely\n",
            "\t --------------------\n",
            "\t My car is so fast\n",
            "\t --------------------\n",
            "\t What a goooooooaaaaaal!!!!!!\n",
            "\t --------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYamrAGT2VEp"
      },
      "source": [
        "Train-Test Split (Or Train Validation Split)\n",
        "\n",
        "Since the testing data doesn't have any labels, we will have to split our training data into train and validation (or test). How about 90-10 for starters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY82QZTT3EZS"
      },
      "source": [
        "# Define test_size\n",
        "test_size = 0.1\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'].to_numpy(), train_df['target'].to_numpy(),\n",
        "                                                                            test_size = test_size,\n",
        "                                                                            random_state = random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3682n5U3tiK",
        "outputId": "56546621-cd3f-46ed-ede5-34b1d65e7a56"
      },
      "source": [
        "# Size of training and validation sets\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sGHCGBk4rqA"
      },
      "source": [
        "## Convert text data into numbers\n",
        "\n",
        "The next step would be to convert string data into numbers. Our labels are alreay numerical, so no need for LabelEncoder, but for tweet text, we surely need to convert.\n",
        "\n",
        "There are 2 major approaches of converting text into numbers - \n",
        "\n",
        "* **Tokenization** - A mapping, or a lookup, for a word/sub-word/character. We can do following types of tokenization - \n",
        "\n",
        "      1. Word-Level Tokenization : Assign every word in the sequence it's own token. For example, in the sentence, 'I Love Pizza', we can assign I:0, Love:2, Pizza:3\n",
        "      2. Character-Level Tokenization : Assign token to every character in the corpus. \n",
        "      3. Sub-word-Level Tokenization : This involves breaking words into parts and assigning tokens to each part. \n",
        "                    For example, 'I love Pineapple', can be broken\n",
        "                    into 'I Lo ve pine app le' and then assign \n",
        "                    tokens to these sub-words. In case of sub-word\n",
        "                    tokenization, a word might have multiple tokens\n",
        "\n",
        "* **Embeddings** - Embedding is a representation of natural language which can be learned. This representation comes in the form of a Feature vector. For example, the word 'Football' can be represented by a 5-d feature vector : [-0.3456, 0.2352, 0.3454, 0.2576, 0.9865]. Importantly, the size of the feature vector is tune-able. We can use embeddings in 2 ways : \n",
        "\n",
        "      1. Create your own embedding : Once your text has been converted to\n",
        "                      numbers (required for Embedding), you can pass this\n",
        "                      to Keras Embedding Layer and an embedding\n",
        "                      representation will be learned during model training\n",
        "      2. Use a pre-learned embedding : `Transfer Learning`. \n",
        "                      The power of Deep Learning. You can use pre-created\n",
        "                      embedding layers and fine-tune them on your\n",
        "                      own purpose. The benefit here is, for example BERT\n",
        "                      is trained on entire wikipedia. It is not possible\n",
        "                      that every time we perform a sentiment analysis we\n",
        "                      train it for months on such a huge corpus. We rather\n",
        "                      use the results from the previous training and\n",
        "                      adapt it to our purpose.\n",
        "\n",
        "\n",
        "To tokenize our words, we'll use the helpful preprocessing layer `tf.keras.layers.experimental.preprocessing.TextVectorization`.\n",
        "\n",
        "The TextVectorization layer takes the following parameters:\n",
        "\n",
        "* max_tokens - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.\n",
        "* standardize - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n",
        "* split - How to split text, default is \"whitespace\" which splits on spaces.\n",
        "* ngrams - How many words to contain per token split, for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
        "* output_mode - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n",
        "* output_sequence_length - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n",
        "* pad_to_max_tokens - If True (default), the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80J0jfKvHIfD",
        "outputId": "1ff561e2-20ca-4eb3-bb36-d1285b3434ad"
      },
      "source": [
        "# What's the number of distinct words in the corpus?\n",
        "from collections import Counter\n",
        "# Instantiate Counter\n",
        "results = Counter()\n",
        "# Convert text to lower case, split on whitespace, and create dict\n",
        "train_df['text'].str.lower().str.split().apply(results.update)\n",
        "# Shape of Dictionary\n",
        "print(len(results.keys()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKCGo_R9H7jz"
      },
      "source": [
        "# Close to 30K if we include test data as well. We'll choose 10K as the vocab_size (or max_tokens)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhU7RlUIIK3u",
        "outputId": "1829df27-8101-461b-9dac-8f097b0ed1bc"
      },
      "source": [
        "# Average tweet length?\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OZiZsRiIR49"
      },
      "source": [
        "# We will choose output_sequence_length as 15, since Average Tweet Length is 15"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUoVB-hI96oV"
      },
      "source": [
        "# Initialize Text Vectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "vocab_size = 10000\n",
        "output_len = 15\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=vocab_size,\n",
        "                                    output_sequence_length = output_len,\n",
        "                                    output_mode = 'int')\n",
        "\n",
        "# Adapt the vectorizer created above to our training data (Adapt is like fit-transform)\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYzbuixXJbLr",
        "outputId": "092faae0-39f8-477e-81c9-12ed25d1e145"
      },
      "source": [
        "# Let's see how this vectorizer is working on few sample text\n",
        "\n",
        "sample_corpus = [['I love Pizza'],\n",
        "                 ['I love Football'],\n",
        "                 ['The dog loves cricket']]\n",
        "text_vectorizer(sample_corpus)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 15), dtype=int64, numpy=\n",
              "array([[   8,  107, 3526,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   8,  107, 1528,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2, 1014, 2401, 3964,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL0N2pFrIY3q"
      },
      "source": [
        "# So, every output is of length 15. Same word gets same token\n",
        "# Also, notice that love and loves get different tokens. Ideally we should convert the word into it's stem and assign tokens to the stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2KUJdO9LJ8k",
        "outputId": "b0bc048f-f8a4-402e-9689-de3b8dff7573"
      },
      "source": [
        "# Explore the vectorizer a bit more\n",
        "\n",
        "# All words in the vocab\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "# First 5 words in the vocab\n",
        "first5_words = words_in_vocab[:5]\n",
        "# Last 5 words in the vocab\n",
        "last5_words = words_in_vocab[-5:]\n",
        "\n",
        "print(\"Total numbers of words in the vocab : \", len(words_in_vocab))\n",
        "print(\"Top 5 most popular words in the vocab : \", first5_words)\n",
        "print(\"Top 5 least popular words in the vocab : \", last5_words)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total numbers of words in the vocab :  10000\n",
            "Top 5 most popular words in the vocab :  ['', '[UNK]', 'the', 'a', 'in']\n",
            "Top 5 least popular words in the vocab :  ['pakthey', 'pakistan\\x89Ûªs', 'pakistans', 'pajamas', 'paints']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6tdyohL6gF"
      },
      "source": [
        "### Create an embedding using Keras Embedding Layer\n",
        "\n",
        "Next, we convert the tokens to an embedding. One major advantage is that an embedding layer can be train-able, so we can update the tokens if we've obtained from text vectorizer\n",
        "\n",
        "Main parameters we are looking for - \n",
        "1. input_dim = Size of the vocabulary, i.e. maximum integer index + 1.\n",
        "2. input_length = Length of input sequences being passed to the embedding (15 here)\n",
        "3. output_dim = Output array size (Map input to this dim. If 100, we get output of size (m, input_len, 100) where m is the number of training examples)\n",
        "\n",
        "Please have a look at the official doc for better understanding - [EmbeddingLayer](https://keras.io/api/layers/core_layers/embedding/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9a5ooONmrO"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding_output = 128\n",
        "\n",
        "embedding = layers.Embedding(input_dim = vocab_size,\n",
        "                             output_dim = embedding_output,\n",
        "                             input_length = output_len)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Os_DGuOFl-",
        "outputId": "a8130f67-c3ef-4c35-8a14-6c62bfaa1b31"
      },
      "source": [
        "# Let's see what exactly this 'thing' is doing\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(\"Original Text : \", random_sentence)\n",
        "\n",
        "# Let's embed this random sentence\n",
        "print(\"\\n\\nEmbedded version\")\n",
        "embed_random_sentence = embedding(text_vectorizer([random_sentence]))\n",
        "embed_random_sentence"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Text :  @susanj357 @msnbc @allinwithchris it's like watching a hostage video sometimes ... But not always ( at least not yet)\n",
            "\n",
            "\n",
            "Embedded version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.0224237 , -0.01849544, -0.03383124, ...,  0.01904192,\n",
              "          0.0362964 , -0.04352295],\n",
              "        [-0.00395179, -0.02574918, -0.03988815, ..., -0.01002262,\n",
              "         -0.01744442,  0.01634537],\n",
              "        [ 0.02749852,  0.02855508, -0.02384728, ..., -0.04388067,\n",
              "         -0.03776541, -0.03515083],\n",
              "        ...,\n",
              "        [ 0.03117278,  0.02976768, -0.01927385, ...,  0.00037569,\n",
              "          0.02694685,  0.00456989],\n",
              "        [-0.01876607, -0.01571644, -0.03176429, ...,  0.04200883,\n",
              "          0.0474278 , -0.04851736],\n",
              "        [ 0.02729014,  0.01529792,  0.00428486, ...,  0.03668069,\n",
              "          0.00289582,  0.01143994]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCVsz3RTQfmx",
        "outputId": "d32ba4f0-d5bb-48c3-9ef7-2679c0912fc0"
      },
      "source": [
        "# Take closer look at tensor shape - <tf.Tensor: shape=(1, 15, 128), dtype=float32\n",
        "# 1 input example. 15 words in every tweet. Or 15 max/min tokens per tweet. 128 dimensional embedding. So, every token gets converted to 128 dims\n",
        "\n",
        "\n",
        "# Check out a single token's embedding\n",
        "embed_random_sentence[0][0], len(embed_random_sentence[0][0])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([ 2.2423696e-02, -1.8495440e-02, -3.3831239e-02, -1.2156606e-02,\n",
              "        -1.6544759e-02,  3.7568185e-02, -4.2519093e-02,  1.9483913e-02,\n",
              "         4.7878396e-02, -3.5590909e-02,  4.1911069e-02,  3.8800906e-02,\n",
              "         3.6043193e-02, -2.5492979e-02,  3.8847458e-02,  4.3636177e-02,\n",
              "        -2.9919863e-02, -9.8289251e-03, -2.4374092e-02,  4.7801148e-02,\n",
              "        -5.8044903e-03, -1.4387120e-02,  4.4034217e-02,  3.4640063e-02,\n",
              "         2.5809918e-02,  4.9005713e-02, -3.5748743e-02, -3.5288107e-02,\n",
              "         2.3989510e-02, -3.9614119e-02,  4.8254017e-02,  3.1943176e-02,\n",
              "        -3.3206381e-02, -2.3017241e-02, -1.9272661e-02,  4.8522022e-02,\n",
              "        -4.0746056e-02, -4.0454794e-02, -3.1227781e-02,  2.1004919e-02,\n",
              "         4.3144811e-02, -3.0185748e-02,  1.1668228e-02,  1.8461857e-02,\n",
              "        -2.2398436e-02,  3.8808789e-02, -2.2201587e-02,  4.5010891e-02,\n",
              "         6.3303933e-03,  2.7893577e-02,  2.8120589e-02,  1.4584769e-02,\n",
              "        -1.0746494e-03, -4.8240878e-02, -5.3413026e-03, -3.0049002e-02,\n",
              "         1.8941436e-02,  5.2860267e-03, -1.7445326e-02,  9.5178857e-03,\n",
              "        -1.4209736e-02, -9.7893178e-05,  9.0403184e-03, -3.0200172e-02,\n",
              "        -4.2794719e-03,  2.2271875e-02,  3.5405304e-02, -7.3134676e-03,\n",
              "        -8.4484927e-03,  9.5587149e-03,  1.8839765e-02, -3.7959419e-02,\n",
              "        -4.9016427e-02,  2.3389686e-02, -1.8452574e-02, -3.5279788e-02,\n",
              "         3.4058858e-02,  3.4305897e-02,  1.7094146e-02,  5.5552498e-03,\n",
              "         2.9404428e-02,  1.8244896e-02, -4.2722788e-02,  4.6761520e-03,\n",
              "        -3.8635395e-02, -4.0031243e-02,  4.8332844e-02, -3.5062838e-02,\n",
              "        -2.4340963e-02, -3.9885949e-02, -3.2000769e-02, -2.5086515e-03,\n",
              "        -1.1399113e-02, -2.9473353e-02,  1.0296941e-02, -2.4042606e-02,\n",
              "        -1.7513417e-02,  3.3002470e-02,  2.0878945e-02,  1.8947314e-02,\n",
              "        -3.9802123e-02, -4.2490102e-02, -1.6187787e-02,  2.4770904e-02,\n",
              "         8.6018331e-03,  1.2528334e-02,  2.6379932e-02, -7.2450526e-03,\n",
              "         3.9591316e-02, -7.1897358e-04,  4.3688547e-02, -1.6469814e-02,\n",
              "        -3.6080927e-04, -4.1848622e-02,  4.3903813e-03,  5.3966045e-03,\n",
              "        -1.0524772e-02,  4.1542675e-02, -1.5081417e-02,  1.8820737e-02,\n",
              "         7.5414404e-03, -5.6047067e-03,  3.6113311e-02, -3.4767222e-02,\n",
              "         2.6601885e-02,  1.9041922e-02,  3.6296401e-02, -4.3522954e-02],\n",
              "       dtype=float32)>, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn2Gb9T7RU3O"
      },
      "source": [
        "# So, every token (or a word if we use word-level tokenization) gets mapped into a 128 dimensional space\n",
        "\n",
        "# What a beautiful matrix. LOL!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnW1EhTHRhdg"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "We'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLEG3Y-eS98G"
      },
      "source": [
        "# Since we will doing a lot of experimentation, it's a good idea we create some base functions\n",
        "# Following function evaluates: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKuckuveR5uo"
      },
      "source": [
        "### Model 0 - Baseline NB Model\n",
        "\n",
        "Just like TF, Scikit Learn models also don't take strings as inputs (DUHH!!)\n",
        "\n",
        "So, we will use [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) score for each word to convert text to number\n",
        "\n",
        "Also, we will create Sklearn pipeline to get the results\n",
        "\n",
        "But before that, a bit about TF-IDF. TF-IDF evaluated how relevant a word is in a collection of documents. It has 2 terms - \n",
        "\n",
        "1. Term Frequency (TF) - This measures the frequency of a word in a doc. We normalise the effect of the words by the total number of words in the doc.\n",
        "\n",
        "> TF = Freq of word W in Doc D / Total number of words in D\n",
        "\n",
        "2. Inverse Document Frequency (IDF) - This measures the importance of a document in the corpus. We are only interested in the number of docs the word W is present and NOT IN THE FREQUENCY.\n",
        "\n",
        "> IDF Definition = Inverse of (Number of docs in which word W occurs / Total number of Docs in Corpus). However, if number of docs in which W occurs is zero, then we might get DivisionByZeroError\n",
        "    \n",
        "So, IDF is defined as - \n",
        "\n",
        "> IDF = log(N / (df + 1)), where df is the number of docs in which W occurs\n",
        "\n",
        "So, putting it all together, TF-IDF score for a word W is -\n",
        "  \n",
        "    TFIDF = tf(w, d) * log(N / (df + 1)\n",
        "            where w : word for which TFIDF is being calculated\n",
        "                  d : current document\n",
        "                 df : number of docs in which w occurs\n",
        "                  N : Total number of docs\n",
        "\n",
        "\n",
        "Notice that TF-IDF normalizes the score for words like in, the, as, so (Stopwords). TF score will be high, but since these words will be present in most of the docs, IDF score will be very low. Still, it's a good practice to perform Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVYk8ZsS3jg",
        "outputId": "abbd6352-7e77-45f1-f669-02ec98722202"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and MNB pipeline\n",
        "model_0 = Pipeline([\n",
        "                    ('tfidf', TfidfVectorizer()),\n",
        "                    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)\n",
        "\n",
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "print(\"Few predictions : \", baseline_preds[:10])\n",
        "\n",
        "# Evaluate the model on Testing data\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")\n",
        "\n",
        "# Evaluation metrics\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few predictions :  [0 0 0 0 0 0 0 0 0 1]\n",
            "Our baseline model achieves an accuracy of: 77.82%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 77.82152230971128,\n",
              " 'f1': 0.7703527809038113,\n",
              " 'precision': 0.792992256322435,\n",
              " 'recall': 0.7782152230971129}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0nb2n6GUWfM"
      },
      "source": [
        "### Model 1 - A simple Dense Model\n",
        "\n",
        "Too simple I'd say. Basically we are taking text and labels as input, do the tokenization, create embedding, convert embedding into lower dimension (like avg) and then pass it to 1 fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgHEguh963gF",
        "outputId": "569a7b4a-b699-4ed8-81d9-06286273612d"
      },
      "source": [
        "# We have 1-d input. Tweet as a string. \n",
        "inputs = layers.Input(shape=(1,), dtype = 'string')\n",
        "# Tokenize the text\n",
        "x = text_vectorizer(inputs)\n",
        "# Create embeddings using embedding layer created above\n",
        "x = embedding(x)\n",
        "# Convert to lower dimesnion, using Average\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "# Create outputs - since the output is Binary, we'll use Sigmoid\n",
        "outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_1 = tf.keras.Model(inputs, outputs, name = 'model_1_dense')\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss = 'binary_crossentropy',\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get a short summary of the model\n",
        "model_1.summary()\n",
        "\n",
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs = 5,\n",
        "                              validation_data = (val_sentences, val_labels),\n",
        "                              callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR,\n",
        "                                                                       experiment_name = 'Simple_Dense_Model')])\n",
        "\n",
        "# Evaluate the model\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 15, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving TensorBoard log files to: model_logs/Simple_Dense_Model/20210717-173539\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 4s 17ms/step - loss: 0.6119 - accuracy: 0.6911 - val_loss: 0.5409 - val_accuracy: 0.7769\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.4402 - accuracy: 0.8215 - val_loss: 0.4845 - val_accuracy: 0.7874\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.3448 - accuracy: 0.8610 - val_loss: 0.4753 - val_accuracy: 0.7848\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.2828 - accuracy: 0.8902 - val_loss: 0.4851 - val_accuracy: 0.7808\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.2352 - accuracy: 0.9117 - val_loss: 0.4992 - val_accuracy: 0.7940\n",
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.49923044443130493, 0.7939632534980774]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVueV9WJjIu9",
        "outputId": "7fc7be0d-f1e8-4b05-b539-d6c99804af6b"
      },
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.4992 - accuracy: 0.7940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.49923044443130493, 0.7939632534980774]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yotrZ9vujLsk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}