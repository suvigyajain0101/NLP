{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_NLP_in_TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQvIYaaBiyZ1K3yj712cbI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suvigyajain0101/NLP/blob/main/Intro_to_NLP_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46QLXRNPdk5f"
      },
      "source": [
        "# But, what is NLP?\n",
        "\n",
        "NLP is a branch of AI, that deals with the interaction between computers and humans using the natural language. Most NLP techniques rely on Machine Learning to make sense of natural language. \n",
        "The ultimate objective of NLP is to read, decipher, understand and make sense of human languages\n",
        "\n",
        "Typical NLP use cases - \n",
        "* Voice assistants, Ok Google, Siri, Cortana\n",
        "* Language Translation, Google translate, Bing\n",
        "* Gmail's spam detection filter\n",
        "* MS Word's grammatical mistake feature\n",
        "\n",
        "There are 2 broad approaches of handling NLP problems - Syntactic and Semantic Analysis\n",
        "\n",
        "1. Syntax - Syntax in a code means the rules that need to followed for that particular programming language. Syntax in a language means the grammatical rules that need to be followed.\n",
        "      Like POS tagging, lemmatization, stemming etc\n",
        "\n",
        "2. Semantic - Semantic analytics dive deeper into the language to understand meaning that is conveyed by the text and sentence structure.\n",
        "      Like NERs, NLU etc\n",
        "\n",
        "\n",
        "Above is a short summary of this [Article](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
        "\n",
        "From machine learning perspective, NLP is about being as creative as we can get with converting our text to numbers. Why is that? Because ML models don't take text as input. In this notebook, we will use the following approach to break an NLP problem and derive insights from it - \n",
        "1. Get the text\n",
        "2. Convert it into numbers\n",
        "3. Modelling\n",
        "4. Predictions\n",
        "\n",
        "\n",
        "We will start with a baseline TF-IDF model and build on it using DNN, LSTM, GRU, Conv and Transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbiyDMHrn91b"
      },
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "# Let's define Universal Random State\n",
        "random_state = 42\n",
        "\n",
        "# For TensorBoard, let's define log storage directory\n",
        "SAVE_DIR = 'model_logs'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJ4nlQDnNbS"
      },
      "source": [
        "#### The below code block can be used to download data from kaggle directly into Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lC1oC9WkSpS"
      },
      "source": [
        "## Data - We will use Kaggle's disaster tweets dataset\n",
        "\n",
        "# Natural Language Processing with Disaster Tweets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQyYirAqlOsE",
        "outputId": "faa54168-5234-4882-b9e9-ada0616483d0"
      },
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 3.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=5a734a17da566577fd9e2f82d3ac979b08009cd53ccc1e568626b895e3476284\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "DVChbXkKlqgf",
        "outputId": "39529f99-d20b-4313-fa74-00c29dd59ec9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1da00e0b-6fc2-42c0-ab69-ad9c43d7c557\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1da00e0b-6fc2-42c0-ab69-ad9c43d7c557\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"suvigyajain\",\"key\":\"82b0c72e48196f9a45aa86528a87c74d\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cg-uesRWmLhz"
      },
      "source": [
        "! mkdir ~/.kaggle\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6zDaCmdmMAZ"
      },
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExGeZ-lLmMHC"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL_JCyCimMMs",
        "outputId": "be18ed44-75e7-4491-ee2a-d9c8476bc579"
      },
      "source": [
        "! kaggle competitions download -c 'nlp-getting-started'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nlp-getting-started.zip to /content\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 95.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTIO6NuimMRz"
      },
      "source": [
        "! mkdir input_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OMwU_AZmMWa",
        "outputId": "d6a65ead-b1bc-4dc0-dd6c-1b158f64c534"
      },
      "source": [
        "! unzip nlp-getting-started.zip -d input_data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  nlp-getting-started.zip\n",
            "  inflating: input_data/sample_submission.csv  \n",
            "  inflating: input_data/test.csv     \n",
            "  inflating: input_data/train.csv    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOhT97ynZh6"
      },
      "source": [
        "### Let's deep dive in the data\n",
        "\n",
        "Kaggle created 3 files for us - train, test and sample-submission\n",
        "We will read the files, do some EDA and then move forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd0V2ICTn0gz",
        "outputId": "65a23d0f-1789-4793-c090-84d20e935eca"
      },
      "source": [
        "train_df = pd.read_csv('input_data/train.csv')\n",
        "test_df = pd.read_csv('input_data/test.csv')\n",
        "\n",
        "print(\"Train Data Size : \", len(train_df))\n",
        "print(\"Test Data Size : \", len(test_df))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Size :  7613\n",
            "Test Data Size :  3263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IrVwgNxtojDI",
        "outputId": "51553647-faf2-4ae3-8734-9d5401b1cc41"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qPct8kRepG0h",
        "outputId": "8d770c25-25b0-44ee-b5e7-1bf4a6161b23"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjbBKJispQFh"
      },
      "source": [
        "# Data desc on Kaggle mentions that the location has this distribution - \n",
        "                  # USA -> 1%\n",
        "                  # RoW -> 65%\n",
        "                  # Null-> 33%"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ze_se3oeqToS",
        "outputId": "3f1096b4-a2bb-457a-a9c4-13b0c3c7086f"
      },
      "source": [
        "# What about keywords?\n",
        "train_df[train_df['keyword'].notnull()].head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>48</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Birmingham</td>\n",
              "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>49</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Est. September 2012 - Bristol</td>\n",
              "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>50</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>AFRICA</td>\n",
              "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>52</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Philadelphia, PA</td>\n",
              "      <td>Crying out for more! Set me ablaze</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>53</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>London, UK</td>\n",
              "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id keyword  ...                                               text target\n",
              "31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n",
              "32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n",
              "33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n",
              "34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n",
              "35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB3RqRCMq0wy",
        "outputId": "40f5d393-841e-4375-ddff-acc8a4aa1e1c"
      },
      "source": [
        "# Are key words always part of the text? If so, then can we just ignore those?\n",
        "train_df[train_df['keyword'].notnull()].apply(lambda x: x.keyword in x.text, axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31       True\n",
              "32      False\n",
              "33       True\n",
              "34       True\n",
              "35      False\n",
              "        ...  \n",
              "7578     True\n",
              "7579     True\n",
              "7580     True\n",
              "7581     True\n",
              "7582     True\n",
              "Length: 7552, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7DOGJyNsGNl"
      },
      "source": [
        "# Not always true!! Let's see in the later stages if we can somehow use this added information"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPWzc21MzTog",
        "outputId": "d07dab51-b96d-4123-b5ea-54559a3331a4"
      },
      "source": [
        "train_df['target'].value_counts()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNJp_-fEzYfC"
      },
      "source": [
        "Another Binary Classification Problem. Fairly balanced target (60-40)\n",
        "\n",
        "  1 - Disaster Related Tweet\n",
        "  \n",
        "  2 - Not related to Disaster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYR13Mz0ze0t",
        "outputId": "0bf4ecc3-88fd-475c-cc76-ad18c5a7b218"
      },
      "source": [
        "# Let's print out few 1s and 0s\n",
        "\n",
        "positive_tweet_sample = train_df[train_df['target'] == 1]['text'].head().tolist()\n",
        "negative_tweet_sample = train_df[train_df['target'] == 0]['text'].head().tolist()\n",
        "\n",
        "\n",
        "print(\"Disaster Related Tweets : \", '\\n')\n",
        "for i in range(5):\n",
        "  print('\\t', positive_tweet_sample[i])\n",
        "  print('\\t', '-'*20)\n",
        "print('\\n')\n",
        "print(\"Non-Disaster Related Tweets : \", '\\n')\n",
        "for i in range(5):\n",
        "  print('\\t', negative_tweet_sample[i])\n",
        "  print('\\t', '-'*20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disaster Related Tweets :  \n",
            "\n",
            "\t Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "\t --------------------\n",
            "\t Forest fire near La Ronge Sask. Canada\n",
            "\t --------------------\n",
            "\t All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "\t --------------------\n",
            "\t 13,000 people receive #wildfires evacuation orders in California \n",
            "\t --------------------\n",
            "\t Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
            "\t --------------------\n",
            "\n",
            "\n",
            "Non-Disaster Related Tweets :  \n",
            "\n",
            "\t What's up man?\n",
            "\t --------------------\n",
            "\t I love fruits\n",
            "\t --------------------\n",
            "\t Summer is lovely\n",
            "\t --------------------\n",
            "\t My car is so fast\n",
            "\t --------------------\n",
            "\t What a goooooooaaaaaal!!!!!!\n",
            "\t --------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYamrAGT2VEp"
      },
      "source": [
        "Train-Test Split (Or Train Validation Split)\n",
        "\n",
        "Since the testing data doesn't have any labels, we will have to split our training data into train and validation (or test). How about 90-10 for starters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY82QZTT3EZS"
      },
      "source": [
        "# Define test_size\n",
        "test_size = 0.1\n",
        "\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df['text'].to_numpy(), train_df['target'].to_numpy(),\n",
        "                                                                            test_size = test_size,\n",
        "                                                                            random_state = random_state)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3682n5U3tiK",
        "outputId": "a468764b-060d-4a73-8e41-d23acefa2911"
      },
      "source": [
        "# Size of training and validation sets\n",
        "len(train_sentences), len(train_labels), len(val_sentences), len(val_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 6851, 762, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sGHCGBk4rqA"
      },
      "source": [
        "## Convert text data into numbers\n",
        "\n",
        "The next step would be to convert string data into numbers. Our labels are alreay numerical, so no need for LabelEncoder, but for tweet text, we surely need to convert.\n",
        "\n",
        "There are 2 major approaches of converting text into numbers - \n",
        "\n",
        "* **Tokenization** - A mapping, or a lookup, for a word/sub-word/character. We can do following types of tokenization - \n",
        "\n",
        "      1. Word-Level Tokenization : Assign every word in the sequence it's own token. For example, in the sentence, 'I Love Pizza', we can assign I:0, Love:2, Pizza:3\n",
        "      2. Character-Level Tokenization : Assign token to every character in the corpus. \n",
        "      3. Sub-word-Level Tokenization : This involves breaking words into parts and assigning tokens to each part. \n",
        "                    For example, 'I love Pineapple', can be broken\n",
        "                    into 'I Lo ve pine app le' and then assign \n",
        "                    tokens to these sub-words. In case of sub-word\n",
        "                    tokenization, a word might have multiple tokens\n",
        "\n",
        "* **Embeddings** - Embedding is a representation of natural language which can be learned. This representation comes in the form of a Feature vector. For example, the word 'Football' can be represented by a 5-d feature vector : [-0.3456, 0.2352, 0.3454, 0.2576, 0.9865]. Importantly, the size of the feature vector is tune-able. We can use embeddings in 2 ways : \n",
        "\n",
        "      1. Create your own embedding : Once your text has been converted to\n",
        "                      numbers (required for Embedding), you can pass this\n",
        "                      to Keras Embedding Layer and an embedding\n",
        "                      representation will be learned during model training\n",
        "      2. Use a pre-learned embedding : `Transfer Learning`. \n",
        "                      The power of Deep Learning. You can use pre-created\n",
        "                      embedding layers and fine-tune them on your\n",
        "                      own purpose. The benefit here is, for example BERT\n",
        "                      is trained on entire wikipedia. It is not possible\n",
        "                      that every time we perform a sentiment analysis we\n",
        "                      train it for months on such a huge corpus. We rather\n",
        "                      use the results from the previous training and\n",
        "                      adapt it to our purpose.\n",
        "\n",
        "\n",
        "To tokenize our words, we'll use the helpful preprocessing layer `tf.keras.layers.experimental.preprocessing.TextVectorization`.\n",
        "\n",
        "The TextVectorization layer takes the following parameters:\n",
        "\n",
        "* max_tokens - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.\n",
        "* standardize - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n",
        "* split - How to split text, default is \"whitespace\" which splits on spaces.\n",
        "* ngrams - How many words to contain per token split, for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
        "* output_mode - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n",
        "* output_sequence_length - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n",
        "* pad_to_max_tokens - If True (default), the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80J0jfKvHIfD",
        "outputId": "d031cb89-528e-4a6d-968b-79d6988d4ef0"
      },
      "source": [
        "# What's the number of distinct words in the corpus?\n",
        "from collections import Counter\n",
        "# Instantiate Counter\n",
        "results = Counter()\n",
        "# Convert text to lower case, split on whitespace, and create dict\n",
        "train_df['text'].str.lower().str.split().apply(results.update)\n",
        "# Shape of Dictionary\n",
        "print(len(results.keys()))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKCGo_R9H7jz"
      },
      "source": [
        "# Close to 30K if we include test data as well. We'll choose 10K as the vocab_size (or max_tokens)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhU7RlUIIK3u",
        "outputId": "237a2a58-4282-4206-a73d-a86c20afd65e"
      },
      "source": [
        "# Average tweet length?\n",
        "round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OZiZsRiIR49"
      },
      "source": [
        "# We will choose output_sequence_length as 15, since Average Tweet Length is 15"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUoVB-hI96oV"
      },
      "source": [
        "# Initialize Text Vectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "vocab_size = 10000\n",
        "output_len = 15\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=vocab_size,\n",
        "                                    output_sequence_length = output_len,\n",
        "                                    output_mode = 'int')\n",
        "\n",
        "# Adapt the vectorizer created above to our training data (Adapt is like fit-transform)\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYzbuixXJbLr",
        "outputId": "640fd5bd-1695-45e3-ec9e-13fe13772e5b"
      },
      "source": [
        "# Let's see how this vectorizer is working on few sample text\n",
        "\n",
        "sample_corpus = [['I love Pizza'],\n",
        "                 ['I love Football'],\n",
        "                 ['The dog loves cricket']]\n",
        "text_vectorizer(sample_corpus)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 15), dtype=int64, numpy=\n",
              "array([[   8,  107, 3526,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   8,  107, 1528,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0],\n",
              "       [   2, 1014, 2401, 3964,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL0N2pFrIY3q"
      },
      "source": [
        "# So, every output is of length 15. Same word gets same token\n",
        "# Also, notice that love and loves get different tokens. Ideally we should convert the word into it's stem and assign tokens to the stem"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2KUJdO9LJ8k",
        "outputId": "ab20317d-a376-4bb7-8936-f9e75dd89bab"
      },
      "source": [
        "# Explore the vectorizer a bit more\n",
        "\n",
        "# All words in the vocab\n",
        "words_in_vocab = text_vectorizer.get_vocabulary()\n",
        "# First 5 words in the vocab\n",
        "first5_words = words_in_vocab[:5]\n",
        "# Last 5 words in the vocab\n",
        "last5_words = words_in_vocab[-5:]\n",
        "\n",
        "print(\"Total numbers of words in the vocab : \", len(words_in_vocab))\n",
        "print(\"Top 5 most popular words in the vocab : \", first5_words)\n",
        "print(\"Top 5 least popular words in the vocab : \", last5_words)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total numbers of words in the vocab :  10000\n",
            "Top 5 most popular words in the vocab :  ['', '[UNK]', 'the', 'a', 'in']\n",
            "Top 5 least popular words in the vocab :  ['pakthey', 'pakistan\\x89Ûªs', 'pakistans', 'pajamas', 'paints']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt6tdyohL6gF"
      },
      "source": [
        "### Create an embedding using Keras Embedding Layer\n",
        "\n",
        "Next, we convert the tokens to an embedding. One major advantage is that an embedding layer can be train-able, so we can update the tokens if we've obtained from text vectorizer\n",
        "\n",
        "Main parameters we are looking for - \n",
        "1. input_dim = Size of the vocabulary, i.e. maximum integer index + 1.\n",
        "2. input_length = Length of input sequences being passed to the embedding (15 here)\n",
        "3. output_dim = Output array size (Map input to this dim. If 100, we get output of size (m, input_len, 100) where m is the number of training examples)\n",
        "\n",
        "Please have a look at the official doc for better understanding - [EmbeddingLayer](https://keras.io/api/layers/core_layers/embedding/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9a5ooONmrO"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "embedding_output = 128\n",
        "\n",
        "embedding = layers.Embedding(input_dim = vocab_size,\n",
        "                             output_dim = embedding_output,\n",
        "                             input_length = output_len)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Os_DGuOFl-",
        "outputId": "43557693-ab9f-4ca8-a23c-893ec0bc1db6"
      },
      "source": [
        "# Let's see what exactly this 'thing' is doing\n",
        "random_sentence = random.choice(train_sentences)\n",
        "print(\"Original Text : \", random_sentence)\n",
        "\n",
        "# Let's embed this random sentence\n",
        "print(\"\\n\\nEmbedded version\")\n",
        "embed_random_sentence = embedding(text_vectorizer([random_sentence]))\n",
        "embed_random_sentence"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Text :  I BET YOU DIDNT KNOW I KICK BOX TOO! https://t.co/rBrw8pWiPJ\n",
            "\n",
            "\n",
            "Embedded version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.01894781, -0.00726347, -0.04992763, ..., -0.0409349 ,\n",
              "         -0.03139122,  0.00509682],\n",
              "        [-0.04105787, -0.02647586,  0.03633232, ..., -0.04589159,\n",
              "         -0.02794155,  0.04608059],\n",
              "        [-0.0431283 , -0.00957646, -0.01949136, ..., -0.03312575,\n",
              "          0.00886799, -0.04914499],\n",
              "        ...,\n",
              "        [-0.02573162, -0.01475918, -0.00649576, ..., -0.00397563,\n",
              "          0.03644766, -0.04618732],\n",
              "        [-0.02573162, -0.01475918, -0.00649576, ..., -0.00397563,\n",
              "          0.03644766, -0.04618732],\n",
              "        [-0.02573162, -0.01475918, -0.00649576, ..., -0.00397563,\n",
              "          0.03644766, -0.04618732]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCVsz3RTQfmx",
        "outputId": "8a5fdb53-f6fd-489b-c7a6-083b53bec410"
      },
      "source": [
        "# Take closer look at tensor shape - <tf.Tensor: shape=(1, 15, 128), dtype=float32\n",
        "# 1 input example. 15 words in every tweet. Or 15 max/min tokens per tweet. 128 dimensional embedding. So, every token gets converted to 128 dims\n",
        "\n",
        "\n",
        "# Check out a single token's embedding\n",
        "embed_random_sentence[0][0], len(embed_random_sentence[0][0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
              " array([ 1.89478137e-02, -7.26347044e-03, -4.99276295e-02,  2.74675004e-02,\n",
              "        -4.64537144e-02, -1.78060159e-02,  3.01951654e-02, -1.20282061e-02,\n",
              "         3.25137414e-02, -2.22574472e-02, -2.61754990e-02,  2.43568160e-02,\n",
              "        -3.20677981e-02, -2.04050783e-02,  1.11190900e-02,  3.04908417e-02,\n",
              "         2.73795463e-02,  4.15088199e-02,  4.29514088e-02, -4.59040664e-02,\n",
              "         8.98696110e-03, -4.43557277e-02,  3.31128575e-02, -1.30199306e-02,\n",
              "        -1.35957375e-02, -1.53317079e-02, -3.20808515e-02,  4.22249548e-02,\n",
              "         2.70626061e-02, -2.92564519e-02,  3.07726301e-02,  3.34253199e-02,\n",
              "         2.40153186e-02,  2.63348483e-02, -1.46042705e-02, -1.15582831e-02,\n",
              "         3.43162306e-02, -2.30682846e-02, -2.15090513e-02, -1.61741488e-02,\n",
              "        -2.90071126e-02, -4.85444330e-02, -1.82237998e-02, -1.39297470e-02,\n",
              "         1.47400834e-02,  4.77270372e-02, -3.42938453e-02, -3.70769128e-02,\n",
              "         3.79866101e-02, -4.37525511e-02, -3.19762006e-02, -4.81764227e-03,\n",
              "        -2.75873020e-03, -1.29029974e-02,  4.62890528e-02,  1.64903514e-02,\n",
              "        -3.94449830e-02,  3.28069068e-02,  3.39879058e-02, -2.55839713e-02,\n",
              "        -4.95825075e-02, -3.92808095e-02, -1.81234367e-02, -1.85073838e-02,\n",
              "        -4.28411029e-02, -1.73840523e-02, -2.22678669e-02, -4.18529026e-02,\n",
              "         1.20128989e-02, -4.17382233e-02,  3.65393795e-02,  1.24336109e-02,\n",
              "         4.28525545e-02,  1.09866746e-02, -4.50398214e-02,  4.98247147e-03,\n",
              "         6.64196908e-04, -3.77661102e-02,  2.88213976e-02,  4.77409363e-03,\n",
              "         9.55600664e-03,  3.73703130e-02, -2.72320509e-02, -3.93017754e-02,\n",
              "        -1.16399042e-02,  2.23023035e-02,  1.24565251e-02, -4.98997234e-02,\n",
              "         3.97797860e-02, -1.08575113e-02, -4.97663617e-02, -9.25160944e-04,\n",
              "         3.88432853e-02,  9.17751715e-03,  3.74700166e-02, -2.18096972e-02,\n",
              "        -3.13551910e-02,  2.57382281e-02,  1.86032318e-02,  7.40350410e-03,\n",
              "        -3.86461616e-05,  6.45480305e-03, -3.38446982e-02,  1.34309791e-02,\n",
              "         4.80837114e-02, -1.91514138e-02,  9.48630646e-03,  3.25324200e-02,\n",
              "        -2.30902676e-02,  3.71321552e-02, -4.21155468e-02, -4.41257358e-02,\n",
              "         3.56631018e-02, -1.75535567e-02,  1.76787712e-02, -2.65626311e-02,\n",
              "        -3.57688665e-02,  4.76674922e-02, -3.16783562e-02,  4.94894274e-02,\n",
              "        -7.79904425e-04, -5.28576225e-03,  6.16929680e-03, -3.66795771e-02,\n",
              "         2.77316011e-02, -4.09348980e-02, -3.13912183e-02,  5.09681553e-03],\n",
              "       dtype=float32)>, 128)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn2Gb9T7RU3O"
      },
      "source": [
        "# So, every token (or a word if we use word-level tokenization) gets mapped into a 128 dimensional space\n",
        "\n",
        "# What a beautiful matrix. LOL!!"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnW1EhTHRhdg"
      },
      "source": [
        "## Modelling\n",
        "\n",
        "We'll be building the following:\n",
        "\n",
        "* Model 0: Naive Bayes (baseline)\n",
        "* Model 1: Feed-forward neural network (dense model)\n",
        "* Model 2: LSTM model\n",
        "* Model 3: GRU model\n",
        "* Model 4: Bidirectional-LSTM model\n",
        "* Model 5: 1D Convolutional Neural Network\n",
        "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
        "* Model 7: Same as model 6 with 10% of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLEG3Y-eS98G"
      },
      "source": [
        "# Since we will doing a lot of experimentation, it's a good idea we create some base functions\n",
        "# Following function evaluates: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results\n",
        "\n",
        "def create_tensorboard_callback(dir_name, experiment_name):\n",
        "  \"\"\"\n",
        "  Creates a TensorBoard callback instand to store log files.\n",
        "  Stores log files with the filepath:\n",
        "    \"dir_name/experiment_name/current_datetime/\"\n",
        "  Args:\n",
        "    dir_name: target directory to store TensorBoard log files\n",
        "    experiment_name: name of experiment directory (e.g. efficientnet_model_1)\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=log_dir\n",
        "  )\n",
        "  print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
        "  return tensorboard_callback\n",
        "\n",
        "def compare_baseline_to_new_results(baseline_results, new_model_results):\n",
        "  \"\"\"\n",
        "  Function to compare results between baseline model and any other model\n",
        "  Both baseline and new model's results are dictionaries of Accuracy, F1, Precision, Recall\n",
        "  \"\"\"\n",
        "  for key, value in baseline_results.items():\n",
        "    print(f\"Baseline {key}: {value:.2f}, New {key}: {new_model_results[key]:.2f}, Difference: {new_model_results[key]-value:.2f}\")"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKuckuveR5uo"
      },
      "source": [
        "### Model 0 - Baseline NB Model\n",
        "\n",
        "Just like TF, Scikit Learn models also don't take strings as inputs (DUHH!!)\n",
        "\n",
        "So, we will use [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) score for each word to convert text to number\n",
        "\n",
        "Also, we will create Sklearn pipeline to get the results\n",
        "\n",
        "But before that, a bit about TF-IDF. TF-IDF evaluated how relevant a word is in a collection of documents. It has 2 terms - \n",
        "\n",
        "1. Term Frequency (TF) - This measures the frequency of a word in a doc. We normalise the effect of the words by the total number of words in the doc.\n",
        "\n",
        "> TF = Freq of word W in Doc D / Total number of words in D\n",
        "\n",
        "2. Inverse Document Frequency (IDF) - This measures the importance of a document in the corpus. We are only interested in the number of docs the word W is present and NOT IN THE FREQUENCY.\n",
        "\n",
        "> IDF Definition = Inverse of (Number of docs in which word W occurs / Total number of Docs in Corpus). However, if number of docs in which W occurs is zero, then we might get DivisionByZeroError\n",
        "    \n",
        "So, IDF is defined as - \n",
        "\n",
        "> IDF = log(N / (df + 1)), where df is the number of docs in which W occurs\n",
        "\n",
        "So, putting it all together, TF-IDF score for a word W is -\n",
        "  \n",
        "    TFIDF = tf(w, d) * log(N / (df + 1)\n",
        "            where w : word for which TFIDF is being calculated\n",
        "                  d : current document\n",
        "                 df : number of docs in which w occurs\n",
        "                  N : Total number of docs\n",
        "\n",
        "\n",
        "Notice that TF-IDF normalizes the score for words like in, the, as, so (Stopwords). TF score will be high, but since these words will be present in most of the docs, IDF score will be very low. Still, it's a good practice to perform Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVYk8ZsS3jg",
        "outputId": "4aa7b96f-7e03-4718-c28f-dce636a5b15a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and MNB pipeline\n",
        "model_0 = Pipeline([\n",
        "                    ('tfidf', TfidfVectorizer()),\n",
        "                    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)\n",
        "\n",
        "# Make predictions\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "print(\"Few predictions : \", baseline_preds[:10])\n",
        "\n",
        "# Evaluate the model on Testing data\n",
        "baseline_score = model_0.score(val_sentences, val_labels)\n",
        "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")\n",
        "\n",
        "# Evaluation metrics\n",
        "baseline_results = calculate_results(y_true=val_labels,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Few predictions :  [0 0 0 0 0 0 0 0 0 1]\n",
            "Our baseline model achieves an accuracy of: 77.82%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 77.82152230971128,\n",
              " 'f1': 0.7703527809038113,\n",
              " 'precision': 0.792992256322435,\n",
              " 'recall': 0.7782152230971129}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0nb2n6GUWfM"
      },
      "source": [
        "### Model 1 - A simple Dense Model\n",
        "\n",
        "Too simple I'd say. Basically we are taking text and labels as input, do the tokenization, create embedding, convert embedding into lower dimension (like avg) and then pass it to 1 fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgHEguh963gF",
        "outputId": "4050b7d8-2d8a-4a25-be8f-cc302f059008"
      },
      "source": [
        "# We have 1-d input. Tweet as a string. \n",
        "inputs = layers.Input(shape=(1,), dtype = 'string')\n",
        "# Tokenize the text\n",
        "x = text_vectorizer(inputs)\n",
        "# Create embeddings using embedding layer created above\n",
        "x = embedding(x)\n",
        "# Convert to lower dimesnion, using Average\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "# Create outputs - since the output is Binary, we'll use Sigmoid\n",
        "outputs = layers.Dense(1, activation = 'sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_1 = tf.keras.Model(inputs, outputs, name = 'model_1_dense')\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss = 'binary_crossentropy',\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "# Get a short summary of the model\n",
        "model_1.summary()\n",
        "\n",
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs = 5,\n",
        "                              validation_data = (val_sentences, val_labels),\n",
        "                              callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR,\n",
        "                                                                       experiment_name = 'Simple_Dense_Model')])\n",
        "\n",
        "# Evaluate the model\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1_dense\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 15, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,280,129\n",
            "Trainable params: 1,280,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Saving TensorBoard log files to: model_logs/Simple_Dense_Model/20210722-183851\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 7s 26ms/step - loss: 0.6115 - accuracy: 0.6894 - val_loss: 0.5419 - val_accuracy: 0.7677\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 5s 25ms/step - loss: 0.4397 - accuracy: 0.8200 - val_loss: 0.4829 - val_accuracy: 0.7927\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 6s 26ms/step - loss: 0.3442 - accuracy: 0.8638 - val_loss: 0.4769 - val_accuracy: 0.7900\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 5s 24ms/step - loss: 0.2816 - accuracy: 0.8904 - val_loss: 0.4849 - val_accuracy: 0.7795\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 5s 25ms/step - loss: 0.2347 - accuracy: 0.9127 - val_loss: 0.5006 - val_accuracy: 0.7795\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 0.5006 - accuracy: 0.7795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5006482601165771, 0.7795275449752808]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVueV9WJjIu9",
        "outputId": "1e18e165-c62f-4f51-8bd0-9b63374cdefc"
      },
      "source": [
        "# Check the results\n",
        "model_1.evaluate(val_sentences, val_labels)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5006482601165771, 0.7795275449752808]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yotrZ9vujLsk",
        "outputId": "904d41a1-d32f-4bc6-9455-57e801873025"
      },
      "source": [
        "# Make predictions (these come back in the form of probabilities)\n",
        "model_1_pred_probs = model_1.predict(val_sentences)\n",
        "model_1_pred_probs[:10] # only print out the first 10 prediction probabilities"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0302527 ],\n",
              "       [0.14803392],\n",
              "       [0.35578954],\n",
              "       [0.02387226],\n",
              "       [0.6122947 ],\n",
              "       [0.13952953],\n",
              "       [0.0191409 ],\n",
              "       [0.26589465],\n",
              "       [0.10997969],\n",
              "       [0.89926887]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MywOJlWfPLd"
      },
      "source": [
        "Sigmoid function returns probabilities and these probabilities need to be converted to 1,0.\n",
        "\n",
        "We will use a simple round function to convert probabilities to binary. This means we use 0.5 as a cut-off. However, for experimentation, we can ROC curve to determine threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkdAQJXWe_Nl",
        "outputId": "0422ca73-0c8d-498e-cc0e-7046da478c71"
      },
      "source": [
        "# Turn prediction probabilities into single-dimension tensor of floats\n",
        "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) # squeeze removes single dimensions\n",
        "model_1_preds[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAWEGd1rfJIi",
        "outputId": "3e00b496-f40f-4015-8c4a-b723b2b514d5"
      },
      "source": [
        "# Calculate model_1 metrics\n",
        "model_1_results = calculate_results(y_true=val_labels, \n",
        "                                    y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 77.95275590551181,\n",
              " 'f1': 0.7763047940993759,\n",
              " 'precision': 0.781906099911785,\n",
              " 'recall': 0.7795275590551181}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUzNGailgfck",
        "outputId": "5a43274d-01fa-4193-e260-4e5282416551"
      },
      "source": [
        "# Is our simple Keras model better than our baseline model?\n",
        "\n",
        "np.array(list(model_1_results.values())) > np.array(list(baseline_results.values()))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False,  True,  True])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TF2t5mahPqm",
        "outputId": "361e6ac0-3161-43ba-fa85-cc0d0778e8c7"
      },
      "source": [
        "# Compare Model's results to Baseline model\n",
        "\n",
        "compare_baseline_to_new_results(baseline_results=baseline_results, \n",
        "                                new_model_results=model_1_results)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 77.82, New accuracy: 77.95, Difference: 0.13\n",
            "Baseline precision: 0.79, New precision: 0.78, Difference: -0.01\n",
            "Baseline recall: 0.78, New recall: 0.78, Difference: 0.00\n",
            "Baseline f1: 0.77, New f1: 0.78, Difference: 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZglOVw0h13B"
      },
      "source": [
        "### RNN\n",
        "\n",
        "Next, we look at another type of ANN architecture - Recurrent Neural Networks. These are based on simple premise of using the information from previous state and define next steps using inputs from the previous state. This is specially useful in problems like Text, Audio etc (basically Sequences). \n",
        "\n",
        "Recurrent neural networks can be used for a number of sequence-based problems:\n",
        "\n",
        "1. **One to one**: one input, one output, such as image classification.\n",
        "2. **One to many**: one input, many outputs, such as image captioning (image input, a sequence of text as caption output).\n",
        "3. **Many to one**: many inputs, one outputs, such as text classification (classifying a Tweet as real diaster or not real diaster).\n",
        "4. **Many to many**: many inputs, many outputs, such as machine translation (translating English to Spanish) or speech to text (audio wave as input, text as output).\n",
        "\n",
        "RNNs can be broadly seen under following variants - \n",
        "\n",
        "1. Long Short Term Memory (LSTM) Units\n",
        "2. Gated Recurrent Units (GRU)\n",
        "3. Bi-Directional RNNs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdwdAIzGhErq"
      },
      "source": [
        "### Model 2 - LSTM\n",
        "\n",
        "LSTM is a type of RNN based on the paradigm of using information from previous state. Long Short Term Memory networks allow the information to persist, unlike traditional neural nets, and can be thought of as multiple copies of the same network. Same can be applied to RNNs. However, as the length of the state grows, i.e., the distance in the past we have to go, RNN become slow. This problem is overcome by LSTMs. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n",
        "\n",
        "\n",
        "The model structure remains pretty same as Model 1, just that we will add an LSTM layer between embedding and output - \n",
        "\n",
        "`Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiASx6GcjPGe"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the input layer\n",
        "inputs = layers.Input(shape=(1,) , dtype = \"string\")\n",
        "\n",
        "# Vectorize the inputs\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "# Convert to embeddings\n",
        "x = embedding(x)\n",
        "\n",
        "# Add LSTM layer\n",
        "x = layers.LSTM(64)(x)\n",
        "\n",
        "# Define output\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_2 = tf.keras.Model(inputs, outputs, name = 'Model_2_LSTM')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LC8yuoZjU9a"
      },
      "source": [
        "# Compile the model\n",
        "model_2.compile(loss = 'binary_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdUfFwsckxP6",
        "outputId": "8481a267-8cee-4ae8-e2be-874c665f2179"
      },
      "source": [
        "# Model Summary\n",
        "model_2.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Model_2_LSTM\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 15, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,329,473\n",
            "Trainable params: 1,329,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cBQmzdPk4ds",
        "outputId": "d13068f1-0a89-4dff-a26b-f1f394674511"
      },
      "source": [
        "# Fit the model\n",
        "\n",
        "model_2.fit(train_sentences,\n",
        "            train_labels,\n",
        "            epochs = 5,\n",
        "            validation_data = (val_sentences, val_labels),\n",
        "            callbacks=[create_tensorboard_callback(SAVE_DIR, \n",
        "                                                    \"LSTM\")])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/LSTM/20210722-185940\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.0718 - accuracy: 0.9707 - val_loss: 0.9110 - val_accuracy: 0.7559\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 7s 35ms/step - loss: 0.0602 - accuracy: 0.9736 - val_loss: 1.3271 - val_accuracy: 0.7467\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 7s 34ms/step - loss: 0.0518 - accuracy: 0.9762 - val_loss: 1.3306 - val_accuracy: 0.7388\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.0432 - accuracy: 0.9796 - val_loss: 1.2830 - val_accuracy: 0.7402\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 8s 35ms/step - loss: 0.0448 - accuracy: 0.9774 - val_loss: 1.4025 - val_accuracy: 0.7388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa67cb8a950>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjdzB7CclS04",
        "outputId": "06216a67-adb1-4c2c-fa45-9e9c496ef125"
      },
      "source": [
        "# Make predictions on the validation dataset\n",
        "model_2_pred_probs = model_2.predict(val_sentences)\n",
        "model_2_pred_probs.shape, model_2_pred_probs[:10] # view the first 10"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((762, 1), array([[7.1352720e-04],\n",
              "        [1.6766280e-02],\n",
              "        [2.7158856e-04],\n",
              "        [9.9003060e-05],\n",
              "        [9.9286509e-01],\n",
              "        [6.8951160e-02],\n",
              "        [9.7978693e-05],\n",
              "        [5.7793856e-03],\n",
              "        [2.2467971e-03],\n",
              "        [9.9994361e-01]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMYdhW7dl19U",
        "outputId": "72684b16-45bc-48d4-e192-ee0ec5c6fd59"
      },
      "source": [
        "# Round out predictions and reduce to 1-dimensional array\n",
        "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
        "model_2_preds[:10]"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9-XnqRsl4Hn",
        "outputId": "3455439b-2486-4706-f640-2982aa312ffd"
      },
      "source": [
        "# Calculate LSTM model results\n",
        "model_2_results = calculate_results(y_true=val_labels,\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 73.88451443569554,\n",
              " 'f1': 0.7376499139456566,\n",
              " 'precision': 0.7378208860176874,\n",
              " 'recall': 0.7388451443569554}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecLmARAEl5uW",
        "outputId": "e350aca4-b0e3-4512-91d2-8b0997b5ce3d"
      },
      "source": [
        "# Compare model 2 to baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_2_results)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 77.82, New accuracy: 73.88, Difference: -3.94\n",
            "Baseline precision: 0.79, New precision: 0.74, Difference: -0.06\n",
            "Baseline recall: 0.78, New recall: 0.74, Difference: -0.04\n",
            "Baseline f1: 0.77, New f1: 0.74, Difference: -0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m3KS_2vl7Tu"
      },
      "source": [
        "### Model 3 - GRU\n",
        "\n",
        "Gated Recurrent Units are similar to LSTM, just have lesser parameters. We will use the same process as LSTMs, where instead of LSTM layer, we will use GRU layer - \n",
        "\n",
        "`Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbMGdpdMm_7l"
      },
      "source": [
        "# Build an RNN using the GRU cell\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the input layer\n",
        "inputs = layers.Input(shape=(1,) , dtype = \"string\")\n",
        "\n",
        "# Vectorize the inputs\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "# Convert to embeddings\n",
        "x = embedding(x)\n",
        "\n",
        "# Add LSTM layer\n",
        "x = layers.GRU(64)(x)\n",
        "\n",
        "# Define output\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_3 = tf.keras.Model(inputs, outputs, name=\"model_3_GRU\")"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrGyeU79nEGx"
      },
      "source": [
        "# Compile GRU model\n",
        "model_3.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etu5uP3XnHwo",
        "outputId": "7cfe00dd-3163-4895-a33e-9240c2a48ece"
      },
      "source": [
        "# Get a summary of the GRU model\n",
        "model_3.summary()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3_GRU\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 15, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 64)                37248     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,317,313\n",
            "Trainable params: 1,317,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcviKCd_nKNZ",
        "outputId": "086ff698-0868-4d8a-90bd-2d01741cd218"
      },
      "source": [
        "# Fit model\n",
        "model_3_history = model_3.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"GRU\")])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/GRU/20210722-190641\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 10s 37ms/step - loss: 0.1353 - accuracy: 0.9479 - val_loss: 0.9215 - val_accuracy: 0.7349\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 7s 33ms/step - loss: 0.0682 - accuracy: 0.9747 - val_loss: 0.8619 - val_accuracy: 0.7664\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 7s 33ms/step - loss: 0.0535 - accuracy: 0.9766 - val_loss: 1.1552 - val_accuracy: 0.7205\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 7s 34ms/step - loss: 0.0435 - accuracy: 0.9788 - val_loss: 1.4458 - val_accuracy: 0.7480\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 7s 33ms/step - loss: 0.0399 - accuracy: 0.9793 - val_loss: 1.5678 - val_accuracy: 0.7467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ismArVmnOrg",
        "outputId": "c3c4f506-83dc-4517-ab8d-862d39577335"
      },
      "source": [
        "# Make predictions on the validation data\n",
        "model_3_pred_probs = model_3.predict(val_sentences)\n",
        "model_3_pred_probs.shape, model_3_pred_probs[:10]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((762, 1), array([[1.8187463e-03],\n",
              "        [7.9035759e-04],\n",
              "        [1.3765693e-04],\n",
              "        [3.3465276e-05],\n",
              "        [9.9695539e-01],\n",
              "        [3.9073646e-02],\n",
              "        [8.3327839e-05],\n",
              "        [2.7772188e-03],\n",
              "        [6.3627958e-04],\n",
              "        [9.9987745e-01]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsMy_8e5nQoj",
        "outputId": "a7e9bfa9-3e39-4c8c-93fe-0591a056c5df"
      },
      "source": [
        "# Convert prediction probabilities to prediction classes\n",
        "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
        "model_3_preds[:10]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_d4gynOnSBP",
        "outputId": "7cf89812-357a-4e70-c141-fd633a318eed"
      },
      "source": [
        "# Calcuate model_3 results\n",
        "model_3_results = calculate_results(y_true=val_labels, \n",
        "                                    y_pred=model_3_preds)\n",
        "model_3_results"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 74.67191601049869,\n",
              " 'f1': 0.7451761796828422,\n",
              " 'precision': 0.7458383161889279,\n",
              " 'recall': 0.7467191601049868}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0qSpC2PnTc_",
        "outputId": "4ce1ac4b-b70f-43e5-dccf-0d7a02663953"
      },
      "source": [
        "# Compare to baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_3_results)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 77.82, New accuracy: 74.67, Difference: -3.15\n",
            "Baseline precision: 0.79, New precision: 0.75, Difference: -0.05\n",
            "Baseline recall: 0.78, New recall: 0.75, Difference: -0.03\n",
            "Baseline f1: 0.77, New f1: 0.75, Difference: -0.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvXPXHZ7nUwD"
      },
      "source": [
        "### Model 4 - Bi-Directional RNNs\n",
        "\n",
        "We have already trained 2 variants of RNNs - LSTM and GRU. Next, we look at another type of RNN - Bi-Directional RNN. \n",
        "\n",
        "A standard RNN will process the data from left to right. Bi-Directional RNN will process the sequence from left-right and then also from right-left. Intuitively, someone reads a sentence from left to right and if it doesn't make much sense, go over the sentence again from right to left. \n",
        "\n",
        "More often than often, Bi-Directional RNNs provide improved performance compared to vanilla RNNs. However, this comes at the cost of longer training times and increased model parameters, since left-right and right-left.\n",
        "\n",
        "Here again, we will follow the same structure as in model 3 and 4 - \n",
        "\n",
        "`Input (text) -> Tokenize -> Embedding -> Layers -> Output (label probability)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4b7xygHpS3F"
      },
      "source": [
        "# Build an RNN using the GRU cell\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define the input layer\n",
        "inputs = layers.Input(shape=(1,) , dtype = \"string\")\n",
        "\n",
        "# Vectorize the inputs\n",
        "x = text_vectorizer(inputs)\n",
        "\n",
        "# Convert to embeddings\n",
        "x = embedding(x)\n",
        "\n",
        "# Add Bi-Directional layer - Notice how we wrapped LSTM layer inside TF Bidirectional. This can be done with any RNN layer\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "\n",
        "# Define output\n",
        "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the model\n",
        "model_4 = tf.keras.Model(inputs, outputs, name=\"model_4_Bidirectional\")"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3LPnctgp-rd"
      },
      "source": [
        "# Compile the model\n",
        "\n",
        "model_4.compile(loss = 'binary_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neStRnkEqMz5",
        "outputId": "fe7c0804-b183-4420-eddd-9bd664630d4b"
      },
      "source": [
        "# Get a summary of our bidirectional model\n",
        "model_4.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4_Bidirectional\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "text_vectorization (TextVect (None, 15)                0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 15, 128)           1280000   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,378,945\n",
            "Trainable params: 1,378,945\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnXQw8_PqTDe"
      },
      "source": [
        "A lot more parameters!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HredtX8qOQz",
        "outputId": "ebe4d6b6-8444-4edc-c226-f581a1c72cfb"
      },
      "source": [
        "# Fit the model (takes longer because of the bidirectional layers)\n",
        "model_4_history = model_4.fit(train_sentences,\n",
        "                              train_labels,\n",
        "                              epochs=5,\n",
        "                              validation_data=(val_sentences, val_labels),\n",
        "                              callbacks=[create_tensorboard_callback(SAVE_DIR, \"bidirectional_RNN\")])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving TensorBoard log files to: model_logs/bidirectional_RNN/20210722-192019\n",
            "Epoch 1/5\n",
            "215/215 [==============================] - 15s 52ms/step - loss: 0.0986 - accuracy: 0.9695 - val_loss: 1.0805 - val_accuracy: 0.7323\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 10s 44ms/step - loss: 0.0411 - accuracy: 0.9809 - val_loss: 1.5130 - val_accuracy: 0.7402\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 10s 46ms/step - loss: 0.0377 - accuracy: 0.9804 - val_loss: 1.3919 - val_accuracy: 0.7480\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 10s 45ms/step - loss: 0.0390 - accuracy: 0.9812 - val_loss: 1.5921 - val_accuracy: 0.7362\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 10s 44ms/step - loss: 0.0397 - accuracy: 0.9799 - val_loss: 1.3411 - val_accuracy: 0.7336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRcLVbPxqWTP",
        "outputId": "27b26dd4-1c70-4f56-cfd8-e7c4a98ae98b"
      },
      "source": [
        "# Make predictions with bidirectional RNN on the validation data\n",
        "model_4_pred_probs = model_4.predict(val_sentences)\n",
        "model_4_pred_probs[:10]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.1818957e-04],\n",
              "       [2.4544120e-02],\n",
              "       [6.1151385e-04],\n",
              "       [8.2206927e-05],\n",
              "       [9.8854160e-01],\n",
              "       [4.6894762e-01],\n",
              "       [5.4006330e-05],\n",
              "       [9.9662095e-02],\n",
              "       [1.8688738e-03],\n",
              "       [9.9977863e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qah_eX1iqX3c",
        "outputId": "229d9f90-178b-485f-bdcf-178714d77851"
      },
      "source": [
        "# Convert prediction probabilities to labels\n",
        "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
        "model_4_preds[:10]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fca84BAKqZIr",
        "outputId": "f49b15b0-5050-448b-e038-20220eebfd41"
      },
      "source": [
        "# Calculate bidirectional RNN model results\n",
        "model_4_results = calculate_results(val_labels, model_4_preds)\n",
        "model_4_results"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 73.35958005249343,\n",
              " 'f1': 0.7335534449181739,\n",
              " 'precision': 0.7335136264316362,\n",
              " 'recall': 0.7335958005249343}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgM3-G4oqaYu",
        "outputId": "bf12a9e0-7388-4011-ec53-1f2755c28c2b"
      },
      "source": [
        "# Check to see how the bidirectional model performs against the baseline\n",
        "compare_baseline_to_new_results(baseline_results, model_4_results)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline accuracy: 77.82, New accuracy: 73.36, Difference: -4.46\n",
            "Baseline precision: 0.79, New precision: 0.73, Difference: -0.06\n",
            "Baseline recall: 0.78, New recall: 0.73, Difference: -0.04\n",
            "Baseline f1: 0.77, New f1: 0.73, Difference: -0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwr7pQh1qb12"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}