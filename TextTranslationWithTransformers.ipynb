{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextTranslationWithTransformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNfIcBEyucEI+xxl3RiZ8NG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model for language understanding\n",
        "\n",
        "This tutorial trains a transformer model to translate a Portuguese to English dataset.\n",
        "\n",
        "This tutorial demonstrates how to build a transformer model and most of its components from scratch using low-level TensorFlow and Keras functionalities. Some of this could be minimized if you took advantage of built-in APIs like tf.keras.layers.MultiHeadAttention.\n",
        "\n",
        "The core idea behind a transformer model is self-attentionâ€”the ability to attend to different positions of the input sequence to compute a representation of that sequence. Transformer creates stacks of self-attention layers and is explained below in the sections Scaled dot product attention and Multi-head attention.\n",
        "\n",
        "A transformer model handles variable-sized input using stacks of self-attention layers instead of RNNs or CNNs. This general architecture has a number of advantages:\n",
        "\n",
        "* It makes no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects (for example, StarCraft units).\n",
        "* Layer outputs can be calculated in parallel, instead of a series like an RNN.\n",
        "* Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see Scene Memory Transformer for example).\n",
        "* It can learn long-range dependencies. This is a challenge in many sequence tasks.\n",
        "\n",
        "\n",
        "The downsides of this architecture are:\n",
        "\n",
        "* For a time-series, the output for a time-step is calculated from the entire history instead of only the inputs and current hidden-state. This may be less efficient.\n",
        "* If the input does have a temporal/spatial relationship, like text, some positional encoding must be added or the model will effectively see a bag of words."
      ],
      "metadata": {
        "id": "FI1mtfgBInls"
      }
    }
  ]
}