{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassificationTensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyON2eYVQ3zZN1rGVoEN2QZG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "paGFo3kOLw4e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, validation_data, test_data), ds_info  = tfds.load('ag_news_subset', \n",
        "                      split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "                      as_supervised=True,\n",
        "                      with_info=True)"
      ],
      "metadata": {
        "id": "ODok7k12QD3N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "_uae1cASQqIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Firstly, let's print dataset info by tensorflow\n",
        "# This is returned by tfds.load constructor. That's pretty cool, btw!"
      ],
      "metadata": {
        "id": "6R8oVbwdVail"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_info"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQp2CqFhVfoK",
        "outputId": "81125de5-8fa4-4093-c516-91517c7f00fb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='ag_news_subset',\n",
              "    version=1.0.0,\n",
              "    description='AG is a collection of more than 1 million news articles.\n",
              "News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity.\n",
              "ComeToMyHead is an academic news search engine which has been running since July, 2004.\n",
              "The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc),\n",
              "information retrieval (ranking, search, etc), xml, data compression, data streaming,\n",
              "and any other non-commercial activity.\n",
              "For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
              "\n",
              "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above.\n",
              "It is used as a text classification benchmark in the following paper:\n",
              "Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
              "\n",
              "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus.\n",
              "Each class contains 30,000 training samples and 1,900 testing samples.\n",
              "The total number of training samples is 120,000 and testing 7,600.',\n",
              "    homepage='https://arxiv.org/abs/1509.01626',\n",
              "    features=FeaturesDict({\n",
              "        'description': Text(shape=(), dtype=tf.string),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=4),\n",
              "        'title': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=127600,\n",
              "    splits={\n",
              "        'test': 7600,\n",
              "        'train': 120000,\n",
              "    },\n",
              "    supervised_keys=('description', 'label'),\n",
              "    citation=\"\"\"@misc{zhang2015characterlevel,\n",
              "        title={Character-level Convolutional Networks for Text Classification},\n",
              "        author={Xiang Zhang and Junbo Zhao and Yann LeCun},\n",
              "        year={2015},\n",
              "        eprint={1509.01626},\n",
              "        archivePrefix={arXiv},\n",
              "        primaryClass={cs.LG}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We have 120000 records for training, and 7600 for testing\n",
        "# Total unique classes are 4 - 0,1,2,3\n",
        "# Let's print first few examples"
      ],
      "metadata": {
        "id": "lW1jeOraV7bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(4)))\n",
        "train_examples_batch, train_labels_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmG8oaAoS89_",
        "outputId": "edf824ab-3138-405f-cd7a-7c9d54ca5581"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.',\n",
              "        b'Reuters - Major League Baseball\\\\Monday announced a decision on the appeal filed by Chicago Cubs\\\\pitcher Kerry Wood regarding a suspension stemming from an\\\\incident earlier this season.',\n",
              "        b'President Bush #39;s  quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.',\n",
              "        b'Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.'],\n",
              "       dtype=object)>,\n",
              " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 1, 2, 3])>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check total examples for train, validation and test"
      ],
      "metadata": {
        "id": "IZFZ2cBLW2LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Total size of the Training dataset : {tf.data.experimental.cardinality(train_data)}')\n",
        "print(f'Total size of the Training dataset : {tf.data.experimental.cardinality(validation_data)}')\n",
        "print(f'Total size of the Training dataset : {tf.data.experimental.cardinality(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgLb_n9sW6DL",
        "outputId": "8429ad81-9ab8-4c8f-e046-7444ae1c5547"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the Training dataset : 72000\n",
            "Total size of the Training dataset : 48000\n",
            "Total size of the Training dataset : 7600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, unique labels. Although this is available in the desc, but still good to know the method!\n",
        "# Since TF datasets are lazily evaluated, the next code block might be slow"
      ],
      "metadata": {
        "id": "MU-u0NmSXsA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text, labels = tuple(zip(*train_data))\n",
        "\n",
        "np_text = np.array(text)\n",
        "np_labels = np.array(labels)\n",
        "\n",
        "print('Unique Labels for training : ', list(set(np_labels)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjCst1sOTiPb",
        "outputId": "f9f01926-ffd3-44c9-b627-c69737b101ee"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels for training :  [0, 1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, few useful TF functions "
      ],
      "metadata": {
        "id": "p8v6YkjYYSWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of unique classes\n",
        "print(f\"No of unique classes : {ds_info.features['label'].num_classes}\")\n",
        "\n",
        "\n",
        "# Get num of examples by the split\n",
        "print(f\"Total training examples : {ds_info.splits['train'].num_examples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URtmZf1LUUDG",
        "outputId": "beb7c109-9592-445a-fab6-930746d3905e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of unique classes : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "bysh7Jl2aLqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding Layer \n",
        "\n",
        "We use an embedding layer for Text Classification. One way to represent the text is to convert sentences into embeddings vectors. \n",
        "Use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "* You don't have to worry about text preprocessing,\n",
        "* Benefit from transfer learning,\n",
        "* The embedding has a fixed size, so it's simpler to process."
      ],
      "metadata": {
        "id": "O6Lqy7AhaPwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
        "                           dtype=tf.string, trainable=True)\n",
        "hub_layer(train_examples_batch[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaDQnmI8ZmRD",
        "outputId": "f77cf633-22c0-41b0-fa8c-4df3132e6ac0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 50), dtype=float32, numpy=\n",
              "array([[ 0.13279007,  0.06140124,  0.1747397 , -0.01384087, -0.00910476,\n",
              "        -0.03726622,  0.07974008,  0.08505542, -0.15469442, -0.07710762,\n",
              "        -0.5860853 ,  0.38640746, -0.17650622, -0.12226384,  0.22213276,\n",
              "         0.37066036,  0.01225427,  0.11542831,  0.20145267,  0.16040364,\n",
              "         0.03724775, -0.1422108 ,  0.04388927,  0.00514791, -0.22648726,\n",
              "        -0.10230689,  0.06203717,  0.09426294,  0.04055819,  0.18911201,\n",
              "         0.2816111 , -0.09024968,  0.04349989, -0.30649066,  0.20486301,\n",
              "        -0.39136994,  0.25492623, -0.06430516,  0.16803294, -0.0635931 ,\n",
              "         0.09554254, -0.05217019, -0.10079663,  0.259143  , -0.16179433,\n",
              "        -0.18240969,  0.05787944,  0.00377896,  0.1353013 ,  0.35294548],\n",
              "       [ 0.2955813 ,  0.134404  ,  0.09672645,  0.1042643 , -0.14633738,\n",
              "         0.21999091, -0.2732706 ,  0.056431  ,  0.3784275 , -0.14614943,\n",
              "         0.0726692 ,  0.12335153,  0.07059986, -0.2501442 ,  0.3267967 ,\n",
              "         0.02605855, -0.31623206,  0.18748201, -0.12888391,  0.10532678,\n",
              "        -0.24986336,  0.01156799, -0.20446077,  0.26960278,  0.09111167,\n",
              "        -0.05465815, -0.24981537,  0.04137783, -0.4020379 ,  0.01403353,\n",
              "        -0.22027768, -0.07405637, -0.00236037, -0.13922532,  0.35359794,\n",
              "        -0.03231022,  0.15417185, -0.11972903, -0.2620559 ,  0.06944627,\n",
              "         0.12570061, -0.16260412,  0.02199288,  0.03206536, -0.0653696 ,\n",
              "        -0.3806908 ,  0.14313512, -0.19247894, -0.08087391, -0.32990122],\n",
              "       [ 0.46365982, -0.18259482,  0.07452042, -0.22763291,  0.03984464,\n",
              "         0.11691552,  0.04256802,  0.28303146,  0.01436664,  0.3757339 ,\n",
              "        -0.10491833,  0.0442509 , -0.26771504, -0.1813014 ,  0.1447292 ,\n",
              "         0.09478033, -0.26464134, -0.26957405, -0.30394343,  0.13095891,\n",
              "        -0.06175926,  0.21836197,  0.329673  , -0.26913914, -0.07300512,\n",
              "         0.26502442, -0.19980146,  0.11361875,  0.11525242, -0.15952322,\n",
              "         0.13240372,  0.11077385,  0.08607626, -0.10145295, -0.14599557,\n",
              "        -0.28842938, -0.22212833, -0.21582195, -0.25874433,  0.05755642,\n",
              "        -0.12010868,  0.27853826,  0.09752402,  0.24491769, -0.00648426,\n",
              "        -0.17682832,  0.13031428,  0.07921226, -0.1383935 ,  0.28922987]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfppvi4vZnI6",
        "outputId": "b55bddc4-283d-4ce2-ea7f-1bf479d54ad3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 50)                48190600  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                816       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 48,191,433\n",
            "Trainable params: 48,191,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BQl4Q8TQZqul"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data.shuffle(10000).batch(512),\n",
        "                    epochs=1,\n",
        "                    validation_data=validation_data.batch(512),\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7N93JypZswe",
        "outputId": "08ed09b0-fbe7-4c41-dde1-aacc25d59d51"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 86s 601ms/step - loss: -4.4591 - accuracy: 0.2629 - val_loss: -18.0566 - val_accuracy: 0.2524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data.batch(512), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lg-FuEYZv_H",
        "outputId": "4e40a81a-1b66-46ee-f877-61ad40033c7d"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 - 1s - loss: -1.8205e+01 - accuracy: 0.2508 - 1s/epoch - 86ms/step\n",
            "loss: -18.205\n",
            "accuracy: 0.251\n"
          ]
        }
      ]
    }
  ]
}